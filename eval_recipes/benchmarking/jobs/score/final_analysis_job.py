# Copyright (c) Microsoft. All rights reserved.

"""Job for consolidating all failure reports for an agent into a single comprehensive report."""

from pathlib import Path

# Import TYPE_CHECKING to avoid circular imports
from typing import TYPE_CHECKING, Any, Literal

from liquid import render
from loguru import logger
from openai.types.shared_params.reasoning import Reasoning
from pydantic import BaseModel, Field

from eval_recipes.benchmarking.job_framework.base import Job, JobContext, JobResult, JobStatus
from eval_recipes.benchmarking.schemas import FinalAnalysisJobInput, FinalAnalysisJobOutput, TrialExecutionJobOutput
from eval_recipes.utils.llm import create_client, truncate_reports_to_token_limit

if TYPE_CHECKING:
    from eval_recipes.benchmarking.jobs.score.trial_execution_job import TrialExecutionJob

FINAL_ANALYSIS_SYSTEM_PROMPT = """\
You are an expert at synthesizing benchmark failure analysis reports and you will be synthesizing across many such reports. \
It is critical that your final report is factual and accurate based on the provided reports. \
Do not make assumptions and inferences that are not directly supported by the reports.

ABOUT BENCHMARKING:
Benchmarking is a way of evaluating AI agents on real-world tasks within sandboxed environments. \
Each task includes a natural language instruction, test scripts for verification, and runs in isolated containers. \
Tasks range from programming to document creation.

Each task can be run multiple times (trials) to assess consistency. \
You will see multiple reports for the same task but different trial numbers. \
This helps identify if failures are consistent or sporadic.

ABOUT THESE REPORTS:
Each individual report was generated by analyzing:
- The task instruction (what the agent was asked to do)
- Agent transcript (full conversation showing all agent actions and tool uses)
- Test output (which tests passed/failed and why)
- Original Dockerfile (environment setup)
- Test files (pytest requirements)

Each report header contains:
- Task name: The benchmark task being evaluated
- Trial number: Which trial run (tasks are run multiple times)

When analyzing, consider:
- Do the same tasks fail consistently across trials? (indicates systemic issues)
- Do some tasks fail sporadically? (indicates non-deterministic problems)

YOUR GOAL:
Identify patterns, common failure modes, and systemic issues across multiple task failures.
Then write a consolidated report summarizing key findings and synthesis about where the agent struggled.

ANALYSIS APPROACH:
- Look for recurring failure patterns (e.g., similar root causes across tasks)
- Identify agent weaknesses (e.g., poor error handling, missing validation, incorrect assumptions)
- Group failures by type (e.g., environment setup issues, logic errors, test misunderstanding)"""

FINAL_ANALYSIS_USER_PROMPT = """\
Analyze the following benchmark task failure reports and create a consolidated analysis.

Your mission: Identify patterns, common failure modes, and systemic issues across all failed tasks.

You will provide two outputs:

1. **full_report**: A detailed markdown report with:
   - Failure categories (group failures by type: environment issues, logic errors, test misunderstanding, etc.)
   - Create up to 5 categories. If there are tasks that don't fit, create an "Other" category.
   - For each category:
     - Count and list the specific tasks in that category
     - Include grounded citations referencing specific task names and trial numbers (e.g., "In task_name_trial_1, the agent failed because...")
     - Common root cause analysis based on the individual reports

2. **executive_summary**: A 1-2 paragraph summary of the full report with:
   - Overall assessment of agent performance
   - Key patterns observed (list of failure categories and counts)

DO NOT include in the full_report:
- A conclusion and appendix
- Recommendations for improvement
- An assessment of the agent's strengths

---

# Individual Task Reports

{{all_reports}}"""


class ConsolidatedReportResponse(BaseModel):
    """Structured output for the consolidated report."""

    full_report: str = Field(
        description="Detailed markdown report with failure categories, grounded citations to specific tasks/trials, and root cause analysis."
    )
    executive_summary: str = Field(
        description="1-2 paragraph summary with overall assessment and key patterns observed."
    )


class FinalAnalysisJob(Job[FinalAnalysisJobOutput]):
    """Job that consolidates all failure reports for an agent into a single comprehensive report.

    Uses a single OpenAI API call with structured outputs to synthesize the reports.
    """

    output_model = FinalAnalysisJobOutput

    def __init__(
        self,
        job_input: FinalAnalysisJobInput,
        trial_execution_jobs: "list[TrialExecutionJob]",
    ) -> None:
        self._input = job_input
        self._trial_execution_jobs = trial_execution_jobs

    @property
    def job_id(self) -> str:
        return f"final_analysis:{self._input.agent_id}"

    @property
    def soft_dependencies(self) -> list[Job[Any]]:
        return list(self._trial_execution_jobs)

    async def run(self, context: JobContext) -> JobResult[FinalAnalysisJobOutput]:
        output_dir: Path = context.config.get("output_dir", Path.cwd() / ".benchmark_results_v2")
        agent_id = self._input.agent_id

        logger.info(f"Starting job: {self.job_id}")

        # Collect failure reports from dependencies (soft deps may have failed)
        reports: list[str] = []
        for job in self._trial_execution_jobs:
            raw_output = context.try_get_output(job)
            if raw_output is None:
                # Job didn't complete successfully, skip it
                continue
            # Cast to TrialExecutionJobOutput for type safety
            output = TrialExecutionJobOutput.model_validate(raw_output.model_dump())
            if not output.analysis_skipped and output.failure_report:
                task_name = job._input.task.name
                trial = job._input.trial_number
                reports.append(f"## Report: {task_name}_trial_{trial}\n\n{output.failure_report}\n\n{'=' * 80}\n")

        # If no failure reports, return early
        if not reports:
            logger.info(f"No failure reports found for agent '{agent_id}', skipping consolidated report")
            return JobResult(
                status=JobStatus.COMPLETED,
                output=FinalAnalysisJobOutput(
                    executive_summary_path="",
                    full_report_path="",
                    executive_summary="",
                    full_report="",
                    report_generated=False,
                    num_reports_analyzed=0,
                ),
            )

        logger.info(f"Consolidating {len(reports)} failure reports for agent '{agent_id}'")

        # Truncate reports to fit within token limit, then combine
        truncated_reports = truncate_reports_to_token_limit(reports)
        all_reports = "\n".join(truncated_reports)

        # Render user prompt with reports
        user_prompt = render(FINAL_ANALYSIS_USER_PROMPT, all_reports=all_reports)

        messages: list = [
            {"role": "system", "content": FINAL_ANALYSIS_SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ]

        try:
            async with create_client(provider=self._input.provider) as client:
                response = await client.responses.parse(
                    model=self._input.model,
                    input=messages,
                    text_format=ConsolidatedReportResponse,
                    reasoning=Reasoning(effort="medium"),
                    store=False,
                )

            if not response.output_parsed:
                error_msg = "Failed to parse consolidated report response"
                logger.error(error_msg)
                return JobResult(status=JobStatus.FAILED, error=error_msg)

            parsed_response: ConsolidatedReportResponse = response.output_parsed

            # Save executive summary and full report separately
            report_dir = output_dir / agent_id
            report_dir.mkdir(parents=True, exist_ok=True)

            executive_summary_path = report_dir / "EXECUTIVE_SUMMARY.md"
            executive_summary_path.write_text(parsed_response.executive_summary, encoding="utf-8")
            logger.info(f"Executive summary saved to: {executive_summary_path}")

            full_report_path = report_dir / "FULL_REPORT.md"
            full_report_path.write_text(parsed_response.full_report, encoding="utf-8")
            logger.info(f"Full report saved to: {full_report_path}")

            return JobResult(
                status=JobStatus.COMPLETED,
                output=FinalAnalysisJobOutput(
                    executive_summary_path=str(executive_summary_path),
                    full_report_path=str(full_report_path),
                    executive_summary=parsed_response.executive_summary,
                    full_report=parsed_response.full_report,
                    report_generated=True,
                    num_reports_analyzed=len(reports),
                ),
            )

        except Exception as e:
            error_msg = f"Failed to generate consolidated report: {e}"
            logger.exception(error_msg)
            return JobResult(status=JobStatus.FAILED, error=error_msg)


async def generate_consolidated_report(
    agent_id: str,
    failure_reports: list[str],
    output_dir: Path,
    provider: Literal["openai", "azure_openai"] = "openai",
    model: str = "gpt-5.2",
) -> FinalAnalysisJobOutput:
    """Standalone function to generate a consolidated report without using the job framework.

    Useful for testing or one-off report generation.

    Args:
        agent_id: The agent identifier
        failure_reports: List of failure report markdown strings
        output_dir: Directory to save the report
        provider: LLM provider to use
        model: Model to use for generation

    Returns:
        FinalAnalysisJobOutput with report details
    """
    if not failure_reports:
        logger.info(f"No failure reports provided for agent '{agent_id}'")
        return FinalAnalysisJobOutput(
            executive_summary_path="",
            full_report_path="",
            executive_summary="",
            full_report="",
            report_generated=False,
            num_reports_analyzed=0,
        )

    logger.info(f"Consolidating {len(failure_reports)} failure reports for agent '{agent_id}'")

    truncated_reports = truncate_reports_to_token_limit(failure_reports)
    all_reports = "\n".join(truncated_reports)

    # Render user prompt with reports
    user_prompt = render(FINAL_ANALYSIS_USER_PROMPT, all_reports=all_reports)

    messages: list = [
        {"role": "system", "content": FINAL_ANALYSIS_SYSTEM_PROMPT},
        {"role": "user", "content": user_prompt},
    ]

    async with create_client(provider=provider) as client:
        response = await client.responses.parse(
            model=model,
            input=messages,
            text_format=ConsolidatedReportResponse,
            reasoning=Reasoning(effort="medium"),
            store=False,
        )

    if not response.output_parsed:
        raise ValueError("Failed to parse consolidated report response")

    parsed_response: ConsolidatedReportResponse = response.output_parsed

    # Save executive summary and full report separately
    report_dir = output_dir / agent_id
    report_dir.mkdir(parents=True, exist_ok=True)

    executive_summary_path = report_dir / "EXECUTIVE_SUMMARY.md"
    executive_summary_path.write_text(parsed_response.executive_summary, encoding="utf-8")
    logger.info(f"Executive summary saved to: {executive_summary_path}")

    full_report_path = report_dir / "FULL_REPORT.md"
    full_report_path.write_text(parsed_response.full_report, encoding="utf-8")
    logger.info(f"Full report saved to: {full_report_path}")

    return FinalAnalysisJobOutput(
        executive_summary_path=str(executive_summary_path),
        full_report_path=str(full_report_path),
        executive_summary=parsed_response.executive_summary,
        full_report=parsed_response.full_report,
        report_generated=True,
        num_reports_analyzed=len(failure_reports),
    )
