{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "MJUe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T20:10:26.181212Z",
     "iopub.status.busy": "2025-08-22T20:10:26.181120Z",
     "iopub.status.idle": "2025-08-22T20:10:26.650862Z",
     "shell.execute_reply": "2025-08-22T20:10:26.650523Z"
    },
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import marimo as mo\n",
    "from openai.types.chat.chat_completion_tool_param import ChatCompletionToolParam\n",
    "from openai.types.responses import (\n",
    "    EasyInputMessageParam,\n",
    "    ResponseFunctionToolCallParam,\n",
    ")\n",
    "from openai.types.responses.response_input_param import FunctionCallOutput\n",
    "\n",
    "from eval_recipes.evaluate import evaluate\n",
    "from eval_recipes.schemas import (\n",
    "    BaseEvaluationConfig,\n",
    "    ClaimVerifierConfig,\n",
    "    GuidanceEvaluationConfig,\n",
    "    ToolEvaluationConfig,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hbol",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Assistant Evaluations\n",
    "\n",
    "Evaluating LLM-based *applications* is hard, this package aims to make it easier, while giving you the flexbility to customize for your unique needs.\n",
    "\n",
    "When you see the word \"evaluations\" in the context of LLMs it could refer to a few things [1]:\n",
    "\n",
    "1. Industry benchmarks for comparing models in isolation, like [MMLU](https://github.com/openai/evals/blob/main/examples/mmlu.ipynb), [ARC-AGI](https://github.com/arcprize/ARC-AGI-2), and those listed on [HuggingFace's leaderboard](https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a)\n",
    "2. Industry benchmarks for comparing specific portions of a system, like memory (ex. [LongMemEval](https://github.com/xiaowu0162/LongMemEval)) or agentic software engineering tasks (ex. [SWE Bench Verified](https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified)).\n",
    "3.  Standard numerical scores like [ROUGE](https://aclanthology.org/W04-1013/) or [BERTScore](https://arxiv.org/abs/1904.09675)\n",
    "4. Specific tests you implement to measure your LLM application's performance.\n",
    "\n",
    "This package is about the last type: providing a set of common \"tests\" to measure your application's performance. We refer to these as **online evaluations** or **telemetry** as they are scores that computed as your application runs, without the need for explict labeled data.\n",
    "In this notebook, we show the four core evaluations we provide: claim verification[2], measuring if user preferences are adhered to, does the assistant provide proper guidance when needed, and does it use tools effectively.\n",
    "\n",
    "The differences between our evaluations and other open source evaluation frameworks are:\n",
    "\n",
    "1. None of our evaluations require any labeled data or criteria specific to a scenario. This enables it to provide signals over novel and messy user scenarios without relying on collecting human labels which are brittle, subjective, and expensive to collect.\n",
    "2. We implement workflows, not simple LLM-as-a-Judge prompts. For example, detecting if user preferences are adhered to is a multi-step process that first extracts user preferences, then validates each of them individually against the conversation history.\n",
    "3. The inputs to each evaluator (our evaluations operate over messages and tool definitions) match one-to-one to the inputs that generated the outputs. This makes it easy to apply and ensures the same context is being considered in the evaluation.\n",
    "\n",
    "## Use Cases\n",
    "There are three core use cases.\n",
    "\n",
    "1. After deployment, monitoring and telemetry.\n",
    "2. Before deployment, testing two different systems against each other. For example, when changing the model used.\n",
    "3. Generating data for model training. For example, one could use the claim verification evaluation outputs to rewrite a response to remove the detected hallucinations.\n",
    "\n",
    "[1] [Evals design best practices - OpenAI](https://platform.openai.com/docs/guides/evals-design?api-mode=responses#what-are-evals).\n",
    "\n",
    "[2] Our claim verification evaluation is based on the following two papers: [Claimify](https://arxiv.org/abs/2502.10855) and [VeriTrail](https://arxiv.org/abs/2505.21786). It is not an official implementation of either and please cite the original papers if you use this evaluation in your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vblA",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T20:10:26.652080Z",
     "iopub.status.busy": "2025-08-22T20:10:26.651924Z",
     "iopub.status.idle": "2025-08-22T20:10:26.680786Z",
     "shell.execute_reply": "2025-08-22T20:10:26.680478Z"
    },
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(_md(), Html(), _md())"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    mo.md(\"# High Level API\"),\n",
    "    mo.image(\"demos/data/Evaluate API Diagram.png\"),\n",
    "    mo.md(\n",
    "        r\"\"\"Assistant evaluations exposes one high level function, `evaluate`, which expects the inputs you are already likely using in your assistant: chat messages and tool definitions. Specifically, we use the same parameters as the [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses/create#responses_create-input). We chose this because it is likely what you are already using and it is a common standard even for other LLM providers to support. It is also straighforward to write your own conversion from any other message and tool types.\n",
    "\"\"\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Example Scenario\n",
    "\n",
    "In the following cell, we define an example scenario. Since the contents are lengthy, we describe it in words here:\n",
    "\n",
    "1. The system prompt encourages the model to call tools, contains a memories, and the assistant is provided with 4 core tools around search and file viewing/editing.\n",
    "2. The user has uploaded a file called \"Earnings Release FY24 Q4.md\" which contains Microsoft's financial data from that quarter.\n",
    "3. They ask for the assistant to read the file and asks for the key metrics.\n",
    "4. The assistant calls the `ls` tool to find the exact file path and then uses the `view` tool to read it\n",
    "5. Finally, the textbox below is a hypothetical message that an assistant can generate. **Note these two important aspects about the default response** for when we look at the final results:\n",
    "    * The responses is bulleted list form, while there is a user memory in the system prompt that suggests to use paragraphs instead\n",
    "    * The responses contains an intentional factual inaccuracy where one of the financial figures was changed from 22.0B to 15.0B.\n",
    "\n",
    "Try to change the response to see how it changes the evaluation results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lEQa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T20:10:26.681772Z",
     "iopub.status.busy": "2025-08-22T20:10:26.681670Z",
     "iopub.status.idle": "2025-08-22T20:10:26.690852Z",
     "shell.execute_reply": "2025-08-22T20:10:26.690553Z"
    },
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<marimo-ui-element object-id='bfc3c925-5705-34ca-4952-91bf74345f4a' random-id='bfc3c925-5705-34ca-4952-91bf74345f4a'><marimo-text-area data-initial-value='&quot;Here are the key metrics from the FY24 Q4 earnings release:&#92;n- Quarterly Revenue: &#36;64.7B, up 15%&#92;n- Operating Income: &#36;27.9B, up 15%&#92;n- Net Income: &#36;15.0B, up 10%&#92;n- Diluted EPS: &#36;2.95, up 10%&quot;' data-label='&quot;&lt;span class=&#92;&quot;markdown prose dark:prose-invert&#92;&quot;&gt;&lt;span class=&#92;&quot;paragraph&#92;&quot;&gt;The will be the final assistant message, which is the focus of the evaluation.&lt;/span&gt;&lt;/span&gt;&quot;' data-placeholder='&quot;&quot;' data-disabled='false' data-debounce='true' data-full-width='true'></marimo-text-area></marimo-ui-element>"
      ],
      "text/markdown": [
       "&lt;marimo-text-area data-initial-value=&#x27;&amp;quot;Here are the key metrics from the FY24 Q4 earnings release:&amp;#92;n- Quarterly Revenue: &amp;#36;64.7B, up 15%&amp;#92;n- Operating Income: &amp;#36;27.9B, up 15%&amp;#92;n- Net Income: &amp;#36;15.0B, up 10%&amp;#92;n- Diluted EPS: &amp;#36;2.95, up 10%&amp;quot;&#x27; data-label=&#x27;&amp;quot;&amp;lt;span class=&amp;#92;&amp;quot;markdown prose dark:prose-invert&amp;#92;&amp;quot;&amp;gt;&amp;lt;span class=&amp;#92;&amp;quot;paragraph&amp;#92;&amp;quot;&amp;gt;The will be the final assistant message, which is the focus of the evaluation.&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;quot;&#x27; data-placeholder=&#x27;&amp;quot;&amp;quot;&#x27; data-disabled=&#x27;false&#x27; data-debounce=&#x27;true&#x27; data-full-width=&#x27;true&#x27;&gt;&lt;/marimo-text-area&gt;"
      ],
      "text/plain": [
       "text_area()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an autonomous agent that helps users get their work done. \\\n",
    "You will be provided a set of tools that you should use to gather context and take actions appropriately according to the user's intent. \\\n",
    "You will be provided memories from previous interactions with the user which you should abide by, unless within the context of this conversation it is not appropriate.\n",
    "\n",
    "<context_gathering>\n",
    "- Search depth: very low\n",
    "- Bias strongly towards providing a correct answer as quickly as possible, even if it might not be fully correct.\n",
    "- Usually, this means an absolute maximum of 2 tool calls.\n",
    "- If you think that you need more time to investigate, update the user with your latest findings and open questions. You can proceed if the user confirms.\n",
    "</context_gathering>\n",
    "\n",
    "<tool_preambles>\n",
    "- Always begin by rephrasing the user's goal in a friendly, clear, and concise manner, before calling any tools.\n",
    "- Then, immediately outline a structured plan detailing each logical step you'll follow.\n",
    "- As you execute your file edit(s), narrate each step succinctly and sequentially, marking progress clearly.\n",
    "- Finish by summarizing completed work distinctly from your upfront plan.\n",
    "</tool_preambles>\n",
    "\n",
    "<memories>\n",
    "- The user prefers responses in paragraph form\n",
    "</memories>\"\"\"\n",
    "\n",
    "CAPABILITY_MANIFEST = \"\"\"# Overview\n",
    "This assistant can work with files available to it, create or edit documents, read file contents, and search for information using the provided tools. \\\n",
    "Its capabilities are limited to what the tools explicitly support and the ability to return textual answers using Markdown syntax\n",
    "\n",
    "## File Operations\n",
    "\n",
    "CAN\n",
    "- Use ls to list the files that you can view.\n",
    "- Use view to read the contents of a file at the specified path.\n",
    "- Use edit_file to create or edit documents.\n",
    "- Extract and summarize information from files it can read (for example, key metrics in an earnings release).\n",
    "\n",
    "CANNOT\n",
    "- Access or read files that are not listed by ls or otherwise not within the accessible file scope.\n",
    "- Delete, move, rename, or copy files or folders (no tool is provided for these operations).\n",
    "- Execute or run files, scripts, or programs (no execution tools are provided).\n",
    "- Guarantee readability of non-text or unsupported formats; if view cannot return readable contents (e.g., binary or protected files), the assistant cannot process them.\n",
    "- Access external storage locations (e.g., cloud drives, email attachments) unless their files are present in the accessible file list.\n",
    "\n",
    "## Data Processing\n",
    "\n",
    "CAN\n",
    "- Read and interpret text returned by view.\n",
    "- Identify, extract, and summarize key metrics and facts present in a document (e.g., revenue, EPS, guidance) when they are explicitly stated in the text.\n",
    "- Perform straightforward comparisons and simple calculations based on the text it can read.\n",
    "\n",
    "CANNOT\n",
    "- Infer or fabricate data that is not present in the viewed content or user-provided context.\n",
    "- Parse or analyze content that is not retrievable as readable text via view.\n",
    "- Perform code execution, data transformation, or advanced analytics beyond text-based reasoning and summarization (no computation or execution tools are provided).\n",
    "\n",
    "## Search and External Information\n",
    "\n",
    "CAN\n",
    "- Use search to search for information.\n",
    "\n",
    "CANNOT\n",
    "- Browse specific URLs, click links, or navigate web pages (no browsing or fetching tool is provided).\n",
    "- Download external files or ingest web content directly into the file workspace.\n",
    "- Access real-time or paywalled data sources beyond what search returns via its own interface.\n",
    "\n",
    "## Writing and Reporting\n",
    "\n",
    "CAN\n",
    "- Generate textual summaries, key takeaways, and bullet lists.\n",
    "- Use edit_file to create or edit documents containing extracted metrics, summaries, notes, or reports.\n",
    "\n",
    "CANNOT\n",
    "- Produce or export files in formats or locations not supported by edit_file.\n",
    "- Guarantee specific document layouts or complex formatting beyond what edit_file supports.\n",
    "\n",
    "## Interaction and Task Flow\n",
    "\n",
    "CAN\n",
    "- Ask for clarification or a file path if the target document (e.g., an earnings release) is not identifiable via ls.\n",
    "- Propose a plan: use ls to discover candidate files, view to inspect contents, then summarize key metrics, and optionally use edit_file to produce a written report.\n",
    "\n",
    "CANNOT\n",
    "- Proceed with file analysis if the needed file is not accessible via ls/view.\n",
    "- Confirm the existence of a file without checking via ls or receiving the exact path from the user.\n",
    "\n",
    "## Constraints and Assumptions\n",
    "\n",
    "- Tool capabilities are limited to their explicit definitions:\n",
    "  - search: Search for information\n",
    "  - edit_file: Create or edit documents\n",
    "  - ls: List the files that you can view\n",
    "  - view: Read the contents of a file at the specified path\n",
    "- The assistant's ability to analyze any document depends on successful retrieval of readable content via view.\n",
    "- Absence of tools for deletion, movement, renaming, code execution, web browsing, downloading, or uploading implies those actions are not supported.\n",
    "\"\"\"\n",
    "\n",
    "SEARCH_TOOL = ChatCompletionToolParam(\n",
    "    type=\"function\",\n",
    "    function={\n",
    "        \"name\": \"search\",\n",
    "        \"description\": \"Search for information\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"query\": {\"type\": \"string\"}},\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "EDIT_TOOL = ChatCompletionToolParam(\n",
    "    type=\"function\",\n",
    "    function={\n",
    "        \"name\": \"edit_file\",\n",
    "        \"description\": \"Create or edit documents\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"path\": {\"type\": \"string\"},\n",
    "                \"content\": {\"type\": \"string\"},\n",
    "            },\n",
    "            \"required\": [\"path\", \"content\"],\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "LS_TOOL = ChatCompletionToolParam(\n",
    "    type=\"function\",\n",
    "    function={\n",
    "        \"name\": \"ls\",\n",
    "        \"description\": \"List the files that you can view\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"path\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"relative path to the root\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"path\"],\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "VIEW_TOOL = ChatCompletionToolParam(\n",
    "    type=\"function\",\n",
    "    function={\n",
    "        \"name\": \"view\",\n",
    "        \"description\": \"Read the contents of a file at the specified path\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"path\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The path to the file to read (e.g., '/docs/file.txt')\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"path\"],\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "USER_MESSAGE = \"Can you load the earnings release file and tell me the key metrics?\"\n",
    "\n",
    "ASSISTANT_MESSAGE_1 = \"You're asking me to load your earnings release file and extract the key metrics. My plan is to first locate the earnings release file in your workspace, then open it to identify and summarize key metrics like revenue, EPS, guidance, segment performance, and any notable YoY/QoQ changes. I will start by listing the directories to find the file, then open the most likely match, and finally present a concise summary of the metrics.\\n\\nStep 1: I will list the top-level directories to locate the earnings release file.\"\n",
    "\n",
    "FUNCTION_CALL_1 = ResponseFunctionToolCallParam(\n",
    "    {\n",
    "        \"type\": \"function_call\",\n",
    "        \"call_id\": \"call_1\",\n",
    "        \"name\": \"ls\",\n",
    "        \"arguments\": \"{'path': '/'}\",\n",
    "    }\n",
    ")\n",
    "\n",
    "FUNCTION_CALL_OUTPUT_1 = FunctionCallOutput(\n",
    "    {\n",
    "        \"type\": \"function_call_output\",\n",
    "        \"call_id\": \"call_1\",\n",
    "        \"output\": \"{'files': ['Earnings Release FY24 Q4.md']}\",\n",
    "    }\n",
    ")\n",
    "\n",
    "FUNCTION_CALL_2 = ResponseFunctionToolCallParam(\n",
    "    {\n",
    "        \"type\": \"function_call\",\n",
    "        \"call_id\": \"call_2\",\n",
    "        \"name\": \"view\",\n",
    "        \"arguments\": \"{'path': 'Earnings Release FY24 Q4.md'}\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Handle both .py and .ipynb environments\n",
    "import sys\n",
    "try:\n",
    "    data_path = Path(__file__).parents[0] / \"data\" / \"Earnings Release FY24 Q4.md\"\n",
    "except NameError:\n",
    "    # Fallback for Jupyter notebooks - use current working directory\n",
    "    data_path = Path.cwd() / \"data\" / \"Earnings Release FY24 Q4.md\"\n",
    "\n",
    "FILE_CONTENT = data_path.read_text(encoding='utf-8')\n",
    "\n",
    "FUNCTION_CALL_OUTPUT_2 = FunctionCallOutput(\n",
    "    {\n",
    "        \"type\": \"function_call_output\",\n",
    "        \"call_id\": \"call_2\",\n",
    "        \"output\": str({\"file_content\": FILE_CONTENT}),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Net income was intentionally made to be 15B instead of 22B\n",
    "DEFAULT_ASSISTANT_MESSAGE = \"\"\"Here are the key metrics from the FY24 Q4 earnings release:\n",
    "- Quarterly Revenue: $64.7B, up 15%\n",
    "- Operating Income: $27.9B, up 15%\n",
    "- Net Income: $15.0B, up 10%\n",
    "- Diluted EPS: $2.95, up 10%\"\"\"\n",
    "\n",
    "final_assistant_message = mo.ui.text_area(\n",
    "    label=\"The will be the final assistant message, which is the focus of the evaluation.\",\n",
    "    value=DEFAULT_ASSISTANT_MESSAGE,\n",
    "    full_width=True,\n",
    ")\n",
    "final_assistant_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "PKri",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T20:10:26.691727Z",
     "iopub.status.busy": "2025-08-22T20:10:26.691647Z",
     "iopub.status.idle": "2025-08-22T20:10:26.693505Z",
     "shell.execute_reply": "2025-08-22T20:10:26.693200Z"
    }
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    EasyInputMessageParam(role=\"system\", content=SYSTEM_PROMPT),\n",
    "    EasyInputMessageParam(role=\"user\", content=USER_MESSAGE),\n",
    "    EasyInputMessageParam(role=\"assistant\", content=ASSISTANT_MESSAGE_1),\n",
    "    FUNCTION_CALL_1,\n",
    "    FUNCTION_CALL_OUTPUT_1,\n",
    "    FUNCTION_CALL_2,\n",
    "    FUNCTION_CALL_OUTPUT_2,\n",
    "    EasyInputMessageParam(role=\"assistant\", content=final_assistant_message.value),\n",
    "]\n",
    "\n",
    "tools: list[ChatCompletionToolParam] = [SEARCH_TOOL, EDIT_TOOL, LS_TOOL, VIEW_TOOL]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xref",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Evaluation Configuration\n",
    "\n",
    "Each evaluation can be configured with specific parameters to control its behavior. All evaluations inherit from `BaseEvaluationConfig` and add their own specific parameters.\n",
    "\n",
    "\n",
    "### Base Configuration\n",
    "\n",
    "All evaluations share these common configuration parameters:\n",
    "\n",
    "- **`provider`** (`\"openai\"` | `\"openai\"`): The LLM provider to use for evaluation.\n",
    "- **`model`** (`\"gpt-5\"` | `\"gpt-5-mini\"` | `\"gpt-5-nano\"` | `\"o3\"` | `\"o4-mini\"`): The model to use for evaluation.\n",
    "\n",
    "\n",
    "### Preference Adherence (`BaseEvaluationConfig`)\n",
    "\n",
    "Uses only the base configuration parameters (`provider` and `model`).\n",
    "\n",
    "\n",
    "### Claim Verification (`ClaimVerifierConfig`)\n",
    "\n",
    "- **`claim_extraction_model`**: The model used to extract claims from text. A smaller, faster model is often sufficient. Default: `\"gpt-5-mini\"`\n",
    "- **`verification_model`**: The model used to verify each extracted claim against source context. Default: `\"gpt-5\"`\n",
    "- **`verification_reasoning_effort`**: The reasoning effort level for verification. It is important that verification is accurate, so using the most capable model is recommended.\n",
    "- **`max_line_length`** (`int`): Maximum character length per line when formatting source documents. Lines longer than this are wrapped. Default: `200`\n",
    "- **`max_concurrency`** (`int`): Sets the maximum number of sentences **and** claims within a sentence that can be processed at once. Default: `1`\n",
    "- **`ignore_tool_names`** (`list[str]`): List of tool names whose outputs should not be used as source context for verification. Useful for excluding tools that return LLM generated content such as generated documents.\n",
    "\n",
    "\n",
    "### Guidance Evaluation (`GuidanceEvaluationConfig`)\n",
    "\n",
    "- **`capability_manifest`**: A description of the assistant's capabilities. This should detail what the assistant can and cannot do. If not provided, it will be auto-generated each time (computationally expensive). **Highly recommended to pre-compute.**\n",
    "- **`in_scope_probability_threshold`** (`float`): The probability threshold (0-100) above which a request is considered within scope. If the probability that a request is in-scope exceeds this threshold, the guidance evaluation is deemed not applicable.\n",
    "\n",
    "\n",
    "### Tool Usage Evaluation (`ToolEvaluationConfig`)\n",
    "\n",
    "- **`tool_thresholds`** (`dict[str, float]`): A dictionary mapping tool names to probability thresholds (0-100). Each threshold indicates the minimum probability at which that specific tool should be called. If a tool's calculated probability exceeds its threshold, it should have been called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "SFPL",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T20:10:26.694308Z",
     "iopub.status.busy": "2025-08-22T20:10:26.694230Z",
     "iopub.status.idle": "2025-08-22T20:10:26.696506Z",
     "shell.execute_reply": "2025-08-22T20:10:26.696154Z"
    }
   },
   "outputs": [],
   "source": [
    "preference_adherence_config = BaseEvaluationConfig(model=\"gpt-5\")\n",
    "\n",
    "claim_verification_config = ClaimVerifierConfig(\n",
    "    claim_extraction_model=\"gpt-5\",\n",
    "    provider=\"openai\",\n",
    "    verification_model=\"gpt-5\",\n",
    "    verification_reasoning_effort=\"medium\",\n",
    "    ignore_tool_names=[\"edit_file\"],\n",
    "    max_concurrency=10,\n",
    ")\n",
    "\n",
    "guidance_config = GuidanceEvaluationConfig(\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-5\",\n",
    "    capability_manifest=CAPABILITY_MANIFEST,\n",
    ")\n",
    "\n",
    "tool_usage_config = ToolEvaluationConfig(\n",
    "    tool_thresholds={\n",
    "        \"search\": 55,\n",
    "        \"edit_file\": 60,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Evaluate\n",
    "\n",
    "`evaluate` will call each individual evaluation you provide in `evaluations`.\n",
    "\n",
    "```python\n",
    "async def evaluate(\n",
    "    messages: ResponseInputParam,\n",
    "    tools: list[ChatCompletionToolParam],\n",
    "    evaluations: list[str],\n",
    "    evaluation_configs: dict[str, BaseEvaluationConfig] = {},\n",
    "    max_concurrency: int = 1,\n",
    ") -> list[EvaluationOutput]:\n",
    "```\n",
    "\n",
    "And it will return a list of:\n",
    "\n",
    "```python\n",
    "class EvaluationOutput(BaseModel):\n",
    "    eval_name: str\n",
    "    applicable: bool\n",
    "    score: float\n",
    "    metadata: dict[str, Any] = {}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "RGSE",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T20:10:26.697278Z",
     "iopub.status.busy": "2025-08-22T20:10:26.697168Z",
     "iopub.status.idle": "2025-08-22T20:12:50.187257Z",
     "shell.execute_reply": "2025-08-22T20:12:50.186804Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 16:10:26.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36meval_recipes.evaluations.claim_verification.claim_verifier\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m263\u001b[0m - \u001b[1mStarting claim verification\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 16:10:26.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36meval_recipes.evaluations.claim_verification.claim_verifier\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m271\u001b[0m - \u001b[1mSplit text into 1 sentences\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 16:10:26.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36meval_recipes.evaluations.claim_verification.claim_verifier\u001b[0m:\u001b[36m_process_sentence\u001b[0m:\u001b[36m309\u001b[0m - \u001b[1mProcessing sentence 1: \"'Here are the key metrics from the FY24 Q4 earnings release:\\n- Quarterly Revenue: $64.7B, up 15%\\n- Op'...\"\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 16:10:56.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36meval_recipes.evaluations.claim_verification.claim_extraction\u001b[0m:\u001b[36m_process_sentence\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSelection result: has_verifiable_claims=True\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 16:11:21.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36meval_recipes.evaluations.claim_verification.claim_extraction\u001b[0m:\u001b[36m_process_sentence\u001b[0m:\u001b[36m100\u001b[0m - \u001b[1mDisambiguation result: 1 disambiguated sentences\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 16:12:19.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36meval_recipes.evaluations.claim_verification.claim_extraction\u001b[0m:\u001b[36m_process_sentence\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mDecomposition result: 8 claims extracted\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 16:12:19.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36meval_recipes.evaluations.claim_verification.claim_extraction\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mClaim extraction complete. Total claims extracted: 8\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 16:12:19.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36meval_recipes.evaluations.claim_verification.claim_verifier\u001b[0m:\u001b[36m_process_sentence\u001b[0m:\u001b[36m323\u001b[0m - \u001b[1mVerifying claim: The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company ...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 16:12:19.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36meval_recipes.evaluations.claim_verification.claim_verifier\u001b[0m:\u001b[36m_process_sentence\u001b[0m:\u001b[36m323\u001b[0m - \u001b[1mVerifying claim: The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company ...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 16:12:19.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36meval_recipes.evaluations.claim_verification.claim_verifier\u001b[0m:\u001b[36m_process_sentence\u001b[0m:\u001b[36m323\u001b[0m - \u001b[1mVerifying claim: The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company ...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 16:12:19.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36meval_recipes.evaluations.claim_verification.claim_verifier\u001b[0m:\u001b[36m_process_sentence\u001b[0m:\u001b[36m323\u001b[0m - \u001b[1mVerifying claim: The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company ...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 16:12:19.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36meval_recipes.evaluations.claim_verification.claim_verifier\u001b[0m:\u001b[36m_process_sentence\u001b[0m:\u001b[36m323\u001b[0m - \u001b[1mVerifying claim: The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company ...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 16:12:19.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36meval_recipes.evaluations.claim_verification.claim_verifier\u001b[0m:\u001b[36m_process_sentence\u001b[0m:\u001b[36m323\u001b[0m - \u001b[1mVerifying claim: The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company ...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 16:12:19.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36meval_recipes.evaluations.claim_verification.claim_verifier\u001b[0m:\u001b[36m_process_sentence\u001b[0m:\u001b[36m323\u001b[0m - \u001b[1mVerifying claim: The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company ...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 16:12:19.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36meval_recipes.evaluations.claim_verification.claim_verifier\u001b[0m:\u001b[36m_process_sentence\u001b[0m:\u001b[36m323\u001b[0m - \u001b[1mVerifying claim: The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company ...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 16:12:50.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36meval_recipes.evaluations.claim_verification.claim_verifier\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m286\u001b[0m - \u001b[1mClaim verification complete. Total claims verified: 8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = await evaluate(\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    evaluations=[\n",
    "        \"guidance\",\n",
    "        \"preference_adherence\",\n",
    "        \"tool_usage\",\n",
    "        \"claim_verification\",\n",
    "    ],\n",
    "    evaluation_configs={\n",
    "        \"guidance\": guidance_config,\n",
    "        \"tool_usage\": tool_usage_config,\n",
    "        \"claim_verification\": claim_verification_config,\n",
    "        \"preference_adherence\": preference_adherence_config,\n",
    "    },\n",
    "    max_concurrency=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "Kclp",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T20:12:50.188533Z",
     "iopub.status.busy": "2025-08-22T20:12:50.188401Z",
     "iopub.status.idle": "2025-08-22T20:12:50.197247Z",
     "shell.execute_reply": "2025-08-22T20:12:50.196942Z"
    },
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"markdown prose dark:prose-invert\"><h2 id=\"summary-of-all-evaluations-run\">Summary of all Evaluations Run</h2>\n",
       "<ul>\n",
       "<li><strong>Applicable Evaluations:</strong> <em>3 out of 4</em></li>\n",
       "<li><strong>Average Score:</strong> <em>64.17%</em></li>\n",
       "</ul>\n",
       "<table>\n",
       "<thead>\n",
       "<tr>\n",
       "<th>Evaluation</th>\n",
       "<th>Applicable</th>\n",
       "<th>Score</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "<td><strong>Guidance</strong></td>\n",
       "<td>No</td>\n",
       "<td>N/A</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td><strong>Preference Adherence</strong></td>\n",
       "<td>Yes</td>\n",
       "<td>5.00%</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td><strong>Tool Usage</strong></td>\n",
       "<td>Yes</td>\n",
       "<td>100.00%</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td><strong>Claim Verification</strong></td>\n",
       "<td>Yes</td>\n",
       "<td>87.50%</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td><strong>Average</strong></td>\n",
       "<td>N/A</td>\n",
       "<td>64.17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>---</td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "<span class=\"paragraph\"><strong>Note:</strong> Because we only had one example, we are averaging across evaluations. Typically you should report across each individual evaluation first in order to analyze each evaluation dimension separately for your assistant.\n",
       "The average score is calculated only from evaluations that were applicable to this example.</span></span>"
      ],
      "text/markdown": [
       "## Summary of all Evaluations Run\n",
       "\n",
       "- **Applicable Evaluations:** *3 out of 4*\n",
       "- **Average Score:** *64.17%*\n",
       "\n",
       "| Evaluation | Applicable | Score |\n",
       "|------------|------------|-------|\n",
       "| **Guidance** | No | N/A |\n",
       "| **Preference Adherence** | Yes | 5.00% |\n",
       "| **Tool Usage** | Yes | 100.00% |\n",
       "| **Claim Verification** | Yes | 87.50% |\n",
       "| **Average** | N/A | 64.17 |\n",
       "---\n",
       "\n",
       "**Note:** Because we only had one example, we are averaging across evaluations. Typically you should report across each individual evaluation first in order to analyze each evaluation dimension separately for your assistant.\n",
       "The average score is calculated only from evaluations that were applicable to this example."
      ],
      "text/plain": [
       "_md()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_data = []\n",
    "for result in results:\n",
    "    summary_data.append(\n",
    "        {\n",
    "            \"name\": result.eval_name.replace(\"_\", \" \").title(),\n",
    "            \"applicable\": result.applicable,\n",
    "            \"score\": result.score if result.applicable else None,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Calculate average score for applicable evaluations\n",
    "applicable_scores = [\n",
    "    item[\"score\"]\n",
    "    for item in summary_data\n",
    "    if item[\"applicable\"] and item[\"score\"] is not None\n",
    "]\n",
    "avg_score = (\n",
    "    sum(applicable_scores) / len(applicable_scores) if applicable_scores else 0\n",
    ")\n",
    "\n",
    "# Build summary table rows\n",
    "summary_rows = []\n",
    "for eval_data in summary_data:\n",
    "    applicability = \"Yes\" if eval_data[\"applicable\"] else \"No\"\n",
    "    score_display = (\n",
    "        f\"{eval_data['score']:.2f}%\"\n",
    "        if eval_data[\"applicable\"] and eval_data[\"score\"] is not None\n",
    "        else \"N/A\"\n",
    "    )\n",
    "\n",
    "    summary_rows.append(\n",
    "        f\"| **{eval_data['name']}** | {applicability} | {score_display} |\"\n",
    "    )\n",
    "\n",
    "summary_table = \"\\n\".join(summary_rows)\n",
    "\n",
    "# Count applicable evaluations\n",
    "num_applicable = sum(1 for item in summary_data if item[\"applicable\"])\n",
    "total_evals = len(summary_data)\n",
    "\n",
    "mo.md(\n",
    "    f\"\"\"\n",
    "## Summary of all Evaluations Run\n",
    "\n",
    "- **Applicable Evaluations:** *{num_applicable} out of {total_evals}*\n",
    "- **Average Score:** *{avg_score:.2f}%*\n",
    "\n",
    "| Evaluation | Applicable | Score |\n",
    "|------------|------------|-------|\n",
    "{summary_table}\n",
    "| **Average** | N/A | {avg_score:.2f} |\n",
    "---\n",
    "\n",
    "**Note:** Because we only had one example, we are averaging across evaluations. Typically you should report across each individual evaluation first in order to analyze each evaluation dimension separately for your assistant.\n",
    "The average score is calculated only from evaluations that were applicable to this example.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "emfo",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T20:12:50.198227Z",
     "iopub.status.busy": "2025-08-22T20:12:50.198133Z",
     "iopub.status.idle": "2025-08-22T20:12:50.200784Z",
     "shell.execute_reply": "2025-08-22T20:12:50.200392Z"
    },
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def render_evaluation_result(\n",
    "    title, result, custom_details=\"\", no_data_message=\"No evaluation performed.\"\n",
    "):\n",
    "    \"\"\"Common function to render evaluation results.\"\"\"\n",
    "    applicable_text = \"yes\" if result.applicable else \"no\"\n",
    "    score_section = (\n",
    "        f\"- **Overall Score: *{result.score:.2f}%***\" if result.applicable else \"\"\n",
    "    )\n",
    "\n",
    "    return f\"\"\"\n",
    "## {title}\n",
    "\n",
    "- **Is the evaluation applicable to this example? *{applicable_text}***\n",
    "{score_section}\n",
    "{custom_details if custom_details else f\"*{no_data_message}*\"}\n",
    "\"\"\"\n",
    "\n",
    "def get_result_by_name(results, eval_name):\n",
    "    \"\"\"Helper to get result by evaluation name.\"\"\"\n",
    "    return next((x for x in results if x.eval_name == eval_name), None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "Hstk",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T20:12:50.201695Z",
     "iopub.status.busy": "2025-08-22T20:12:50.201602Z",
     "iopub.status.idle": "2025-08-22T20:12:50.208424Z",
     "shell.execute_reply": "2025-08-22T20:12:50.208034Z"
    },
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"markdown prose dark:prose-invert\"><h2 id=\"user-preferences-results\">User Preferences Results</h2>\n",
       "<ul>\n",
       "<li><strong>Is the evaluation applicable to this example? <em>yes</em></strong></li>\n",
       "<li><strong>Overall Score: <em>5.00%</em></strong></li>\n",
       "</ul>\n",
       "<h3 id=\"metadata\">Metadata</h3>\n",
       "<span class=\"paragraph\">The user preference evaluation extracts preferences from the conversation and evaluates adherence to each one.\n",
       "For each preference:</span>\n",
       "<ul>\n",
       "<li>The model provides reasoning about whether the preference was followed</li>\n",
       "<li>Generates an adherence probability (0-100%) indicating how well the preference was followed</li>\n",
       "<li>Makes a determination: \"adhered\", \"did_not_adhere\", or \"not_applicable\"</li>\n",
       "</ul>\n",
       "<span class=\"paragraph\">The final score is the average of adherence probabilities for applicable preferences only.</span>\n",
       "<span class=\"paragraph\"><strong>Preference 1</strong>: <em>The user prefers responses in paragraph form</em></span>\n",
       "<ul>\n",
       "<li>Reasoning: <em>The assistant’s last response presented the key metrics as a bulleted list rather than a paragraph. Since the stored user preference explicitly requests paragraph form, and summarizing metrics can be delivered in a paragraph, the response did not follow the preference. There is no contextual reason making the preference inapplicable here.</em></li>\n",
       "<li>Adherence Probability: <em>5.0%</em></li>\n",
       "<li>Determination: <em>did_not_adhere</em></li>\n",
       "</ul></span>"
      ],
      "text/markdown": [
       "## User Preferences Results\n",
       "\n",
       "- **Is the evaluation applicable to this example? *yes***\n",
       "- **Overall Score: *5.00%***\n",
       "### Metadata\n",
       "\n",
       "The user preference evaluation extracts preferences from the conversation and evaluates adherence to each one.\n",
       "For each preference:\n",
       "\n",
       "* The model provides reasoning about whether the preference was followed\n",
       "* Generates an adherence probability (0-100%) indicating how well the preference was followed\n",
       "* Makes a determination: \"adhered\", \"did_not_adhere\", or \"not_applicable\"\n",
       "\n",
       "The final score is the average of adherence probabilities for applicable preferences only.\n",
       "\n",
       "\n",
       "**Preference 1**: *The user prefers responses in paragraph form*\n",
       "\n",
       "- Reasoning: *The assistant’s last response presented the key metrics as a bulleted list rather than a paragraph. Since the stored user preference explicitly requests paragraph form, and summarizing metrics can be delivered in a paragraph, the response did not follow the preference. There is no contextual reason making the preference inapplicable here.*\n",
       "- Adherence Probability: *5.0%*\n",
       "- Determination: *did_not_adhere*"
      ],
      "text/plain": [
       "_md()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prefs_result = get_result_by_name(results, \"preference_adherence\")\n",
    "\n",
    "# Extract adherence details from metadata\n",
    "adherence_list = user_prefs_result.metadata.get(\"adherence_to_preferences\", [])\n",
    "\n",
    "# Build the markdown for each preference\n",
    "preference_sections = []\n",
    "for idx, adherence in enumerate(adherence_list):\n",
    "    preference_sections.append(f\"\"\"\n",
    "**Preference {idx + 1}**: *{adherence[\"preference\"]}*\n",
    "\n",
    "- Reasoning: *{adherence[\"reasoning\"]}*\n",
    "- Adherence Probability: *{adherence[\"adherence_probability\"]:.1f}%*\n",
    "- Determination: *{adherence[\"determination\"]}*\"\"\")\n",
    "\n",
    "preferences_md = (\n",
    "    \"\\n---\\n\".join(preference_sections)\n",
    "    if preference_sections\n",
    "    else \"No preferences evaluated.\"\n",
    ")\n",
    "\n",
    "# Build the details section only if applicable\n",
    "up_details = \"\"\n",
    "if user_prefs_result.applicable:\n",
    "    up_details = f\"\"\"### Metadata\n",
    "\n",
    "The user preference evaluation extracts preferences from the conversation and evaluates adherence to each one.\n",
    "For each preference:\n",
    "\n",
    "* The model provides reasoning about whether the preference was followed\n",
    "* Generates an adherence probability (0-100%) indicating how well the preference was followed\n",
    "* Makes a determination: \"adhered\", \"did_not_adhere\", or \"not_applicable\"\n",
    "\n",
    "The final score is the average of adherence probabilities for applicable preferences only.\n",
    "\n",
    "{preferences_md}\"\"\"\n",
    "\n",
    "mo.md(\n",
    "    render_evaluation_result(\n",
    "        \"User Preferences Results\",\n",
    "        user_prefs_result,\n",
    "        custom_details=up_details,\n",
    "        no_data_message=\"No preferences to evaluate or evaluation not applicable.\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nWHF",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T20:12:50.209295Z",
     "iopub.status.busy": "2025-08-22T20:12:50.209215Z",
     "iopub.status.idle": "2025-08-22T20:12:50.217560Z",
     "shell.execute_reply": "2025-08-22T20:12:50.217254Z"
    },
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"markdown prose dark:prose-invert\"><h2 id=\"guidance-results\">Guidance Results</h2>\n",
       "<ul>\n",
       "<li><strong>Is the evaluation applicable to this example? <em>no</em></strong></li>\n",
       "</ul>\n",
       "<h3 id=\"metadata\">Metadata</h3>\n",
       "<span class=\"paragraph\">The guidance evaluation checks how gracefully an assistant handles out-of-scope requests.\n",
       "It first determines if requests are within capabilities through a multi-step reasoning process</span>\n",
       "<ul>\n",
       "<li>Capabilities Analysis: detailed reasoning about the capabilities available to accomplish the user's task</li>\n",
       "<li>Sufficient Capabilities: whether the available capabilities are sufficient to accomplish the user's task, including the capabilities that could be included to more efficiently or effectively accomplish the task</li>\n",
       "<li>In-Scope Reasoning: whether the assistant could accomplish the user's task with the available capabilities to a high degree</li>\n",
       "</ul>\n",
       "<span class=\"paragraph\">Then, after those reasoning steps, a probability is generated.</span>\n",
       "<span class=\"paragraph\">These were the results for this evaluation:</span>\n",
       "<span class=\"paragraph\"><strong>Capabilities Analysis:</strong> <em>The user wants the assistant to load an earnings release file and extract key metrics. The capability manifest provides tools to handle local files: ls to discover files, view to read file contents, and edit_file to create/edit documents. The assistant can extract and summarize key metrics from readable text. It can ask for a file path if not identifiable via ls and propose a plan to locate and analyze the file.</em></span>\n",
       "<span class=\"paragraph\"><strong>Sufficient Capabilities:</strong> <em>If the earnings release file is within the accessible file scope and is readable via view (i.e., text-based or supported format), the existing capabilities (ls + view + text summarization) are sufficient to load and extract key metrics. Additional capabilities that could improve performance include: (1) more robust file parsing for non-text/binary formats (e.g., PDFs with images/OCR), (2) the ability to access external locations or ingest uploads if the file isn’t present, and (3) web browsing/downloading to fetch the release if not locally available. However, for the stated task within the accessible file set, current tools are adequate.</em></span>\n",
       "<span class=\"paragraph\"><strong>In-Scope Reasoning:</strong> <em>The assistant can perform ls to locate the earnings release, use view to read it, and then summarize key metrics (e.g., revenue, EPS, guidance) explicitly present in the document. It may need to ask for the file path or confirm availability. The only limitations are if the file is not accessible or the format is unreadable via view. Otherwise, it can accomplish the request to a high degree.</em></span>\n",
       "<span class=\"paragraph\"><strong>In-Scope Probability:</strong> <em>85.0%</em></span>\n",
       "<span class=\"paragraph\">If the request was deemed to be <strong>not</strong> in scope, then an LLM evaluates how well the assistant handled that request.</span></span>"
      ],
      "text/markdown": [
       "## Guidance Results\n",
       "\n",
       "- **Is the evaluation applicable to this example? *no***\n",
       "\n",
       "\n",
       "### Metadata\n",
       "\n",
       "The guidance evaluation checks how gracefully an assistant handles out-of-scope requests.\n",
       "It first determines if requests are within capabilities through a multi-step reasoning process\n",
       "\n",
       "* Capabilities Analysis: detailed reasoning about the capabilities available to accomplish the user's task\n",
       "* Sufficient Capabilities: whether the available capabilities are sufficient to accomplish the user's task, including the capabilities that could be included to more efficiently or effectively accomplish the task\n",
       "* In-Scope Reasoning: whether the assistant could accomplish the user's task with the available capabilities to a high degree\n",
       "\n",
       "Then, after those reasoning steps, a probability is generated.\n",
       "\n",
       "These were the results for this evaluation:\n",
       "\n",
       "**Capabilities Analysis:** *The user wants the assistant to load an earnings release file and extract key metrics. The capability manifest provides tools to handle local files: ls to discover files, view to read file contents, and edit_file to create/edit documents. The assistant can extract and summarize key metrics from readable text. It can ask for a file path if not identifiable via ls and propose a plan to locate and analyze the file.*\n",
       "\n",
       "**Sufficient Capabilities:** *If the earnings release file is within the accessible file scope and is readable via view (i.e., text-based or supported format), the existing capabilities (ls + view + text summarization) are sufficient to load and extract key metrics. Additional capabilities that could improve performance include: (1) more robust file parsing for non-text/binary formats (e.g., PDFs with images/OCR), (2) the ability to access external locations or ingest uploads if the file isn’t present, and (3) web browsing/downloading to fetch the release if not locally available. However, for the stated task within the accessible file set, current tools are adequate.*\n",
       "\n",
       "**In-Scope Reasoning:** *The assistant can perform ls to locate the earnings release, use view to read it, and then summarize key metrics (e.g., revenue, EPS, guidance) explicitly present in the document. It may need to ask for the file path or confirm availability. The only limitations are if the file is not accessible or the format is unreadable via view. Otherwise, it can accomplish the request to a high degree.*\n",
       "\n",
       "**In-Scope Probability:** *85.0%*\n",
       "\n",
       "If the request was deemed to be **not** in scope, then an LLM evaluates how well the assistant handled that request."
      ],
      "text/plain": [
       "_md()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guidance_result = get_result_by_name(results, \"guidance\")\n",
    "\n",
    "if guidance_result:\n",
    "    # Extract guidance metadata\n",
    "    in_scope_reasoning = guidance_result.metadata.get(\"in_scope_reasoning\", {})\n",
    "    reasoning = guidance_result.metadata.get(\"reasoning\", \"No reasoning provided.\")\n",
    "\n",
    "    # Build the detailed sections\n",
    "    g_details = \"\"\n",
    "\n",
    "    if in_scope_reasoning:\n",
    "        g_details = f\"\"\"\n",
    "### Metadata\n",
    "\n",
    "The guidance evaluation checks how gracefully an assistant handles out-of-scope requests.\n",
    "It first determines if requests are within capabilities through a multi-step reasoning process\n",
    "\n",
    "* Capabilities Analysis: detailed reasoning about the capabilities available to accomplish the user's task\n",
    "* Sufficient Capabilities: whether the available capabilities are sufficient to accomplish the user's task, including the capabilities that could be included to more efficiently or effectively accomplish the task\n",
    "* In-Scope Reasoning: whether the assistant could accomplish the user's task with the available capabilities to a high degree\n",
    "\n",
    "Then, after those reasoning steps, a probability is generated.\n",
    "\n",
    "These were the results for this evaluation:\n",
    "\n",
    "**Capabilities Analysis:** *{in_scope_reasoning.get(\"capabilities_analysis\", \"N/A\")}*\n",
    "\n",
    "**Sufficient Capabilities:** *{in_scope_reasoning.get(\"sufficient_capabilities\", \"N/A\")}*\n",
    "\n",
    "**In-Scope Reasoning:** *{in_scope_reasoning.get(\"in_scope_reasoning\", \"N/A\")}*\n",
    "\n",
    "**In-Scope Probability:** *{in_scope_reasoning.get(\"is_in_scope_probability\", 0):.1f}%*\n",
    "\n",
    "If the request was deemed to be **not** in scope, then an LLM evaluates how well the assistant handled that request.\"\"\"\n",
    "\n",
    "mo.md(\n",
    "    render_evaluation_result(\n",
    "        \"Guidance Results\",\n",
    "        guidance_result,\n",
    "        custom_details=g_details,\n",
    "        no_data_message=\"No guidance evaluation performed (request was in-scope).\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "iLit",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T20:12:50.218369Z",
     "iopub.status.busy": "2025-08-22T20:12:50.218271Z",
     "iopub.status.idle": "2025-08-22T20:12:50.247272Z",
     "shell.execute_reply": "2025-08-22T20:12:50.246930Z"
    },
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"markdown prose dark:prose-invert\"><h2 id=\"claim-verification-results\">Claim Verification Results</h2>\n",
       "<ul>\n",
       "<li><strong>Is the evaluation applicable to this example? <em>yes</em></strong></li>\n",
       "<li><strong>Overall Score: <em>87.50%</em></strong></li>\n",
       "</ul>\n",
       "<h3 id=\"claim-statistics\">Claim Statistics</h3>\n",
       "<ul>\n",
       "<li><strong>Total Claims:</strong> <em>8</em></li>\n",
       "<li><strong>Supported Claims:</strong> <em>7</em></li>\n",
       "<li><strong>Open Domain Claims:</strong> <em>0</em></li>\n",
       "<li><strong>Unsupported Claims:</strong> <em>1</em></li>\n",
       "</ul>\n",
       "<hr />\n",
       "<h3 id=\"detailed-claim-analysis\">Detailed Claim Analysis</h3>\n",
       "<span class=\"paragraph\"><em>NOTE: Citations are not included in this output, see the dedicated notebook for more details</em></span>\n",
       "<span class=\"paragraph\"><strong>Claim 1:</strong> <em>The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company is not identified] reports revenue of $64.7 billion.</em></span>\n",
       "<ul>\n",
       "<li><strong>Status:</strong> ✅ Supported</li>\n",
       "<li><strong>Original Sentence:</strong></li>\n",
       "</ul>\n",
       "<div class=\"language-markdown codehilite\"><pre><span></span><code> Here are the key metrics from the FY24 Q4 earnings release:\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Quarterly Revenue: $64.7B, up 15%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Operating Income: $27.9B, up 15%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Net Income: $15.0B, up 10%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Diluted EPS: $2.95, up 10%\n",
       "</code></pre></div>\n",
       "<ul>\n",
       "<li><strong>Proof:</strong> <em>The FY24 Q4 earnings release file explicitly states for the quarter ended June 30, 2024: “Revenue was $64.7 billion and increased 15% (up 16% in constant currency).” This directly supports that the earnings release reports revenue of $64.7 billion.</em></li>\n",
       "</ul>\n",
       "<hr />\n",
       "<span class=\"paragraph\"><strong>Claim 2:</strong> <em>The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company is not identified] reports that revenue was up 15% [relative to an unspecified prior period].</em></span>\n",
       "<ul>\n",
       "<li><strong>Status:</strong> ✅ Supported</li>\n",
       "<li><strong>Original Sentence:</strong></li>\n",
       "</ul>\n",
       "<div class=\"language-markdown codehilite\"><pre><span></span><code> Here are the key metrics from the FY24 Q4 earnings release:\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Quarterly Revenue: $64.7B, up 15%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Operating Income: $27.9B, up 15%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Net Income: $15.0B, up 10%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Diluted EPS: $2.95, up 10%\n",
       "</code></pre></div>\n",
       "<ul>\n",
       "<li><strong>Proof:</strong> <em>Supported. The FY24 Q4 earnings release text states: “Revenue was $64.7 billion and increased 15% (up 16% in constant currency)” in the section “for the quarter ended June 30, 2024, as compared to the corresponding period of last fiscal year,” and the file is titled “Earnings Release FY24 Q4.” This directly supports that the FY24 Q4 earnings release reports revenue up 15%.</em></li>\n",
       "</ul>\n",
       "<hr />\n",
       "<span class=\"paragraph\"><strong>Claim 3:</strong> <em>The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company is not identified] reports operating income of $27.9 billion.</em></span>\n",
       "<ul>\n",
       "<li><strong>Status:</strong> ✅ Supported</li>\n",
       "<li><strong>Original Sentence:</strong></li>\n",
       "</ul>\n",
       "<div class=\"language-markdown codehilite\"><pre><span></span><code> Here are the key metrics from the FY24 Q4 earnings release:\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Quarterly Revenue: $64.7B, up 15%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Operating Income: $27.9B, up 15%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Net Income: $15.0B, up 10%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Diluted EPS: $2.95, up 10%\n",
       "</code></pre></div>\n",
       "<ul>\n",
       "<li><strong>Proof:</strong> <em>The FY24 Q4 earnings release text explicitly states: “Operating income was $27.9 billion and increased 15% (up 16% in constant currency),” confirming that operating income reported for the quarter was $27.9 billion. The same document header identifies it as “Earnings Release FY24 Q4.”</em></li>\n",
       "</ul>\n",
       "<hr />\n",
       "<span class=\"paragraph\"><strong>Claim 4:</strong> <em>The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company is not identified] reports that operating income was up 15% [relative to an unspecified prior period].</em></span>\n",
       "<ul>\n",
       "<li><strong>Status:</strong> ✅ Supported</li>\n",
       "<li><strong>Original Sentence:</strong></li>\n",
       "</ul>\n",
       "<div class=\"language-markdown codehilite\"><pre><span></span><code> Here are the key metrics from the FY24 Q4 earnings release:\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Quarterly Revenue: $64.7B, up 15%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Operating Income: $27.9B, up 15%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Net Income: $15.0B, up 10%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Diluted EPS: $2.95, up 10%\n",
       "</code></pre></div>\n",
       "<ul>\n",
       "<li><strong>Proof:</strong> <em>The loaded file list shows a document named “Earnings Release FY24 Q4.md,” indicating the FY24 Q4 earnings release. In the file content, it states: “Operating income was $27.9 billion and increased 15% (up 16% in constant currency)” for the quarter ended June 30, 2024, compared to the corresponding period of last fiscal year. This supports the claim that the FY24 Q4 earnings release reports operating income was up 15%.</em></li>\n",
       "</ul>\n",
       "<hr />\n",
       "<span class=\"paragraph\"><strong>Claim 5:</strong> <em>The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company is not identified] reports net income of $15.0 billion.</em></span>\n",
       "<ul>\n",
       "<li><strong>Status:</strong> ❌ Unsupported</li>\n",
       "<li><strong>Original Sentence:</strong></li>\n",
       "</ul>\n",
       "<div class=\"language-markdown codehilite\"><pre><span></span><code> Here are the key metrics from the FY24 Q4 earnings release:\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Quarterly Revenue: $64.7B, up 15%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Operating Income: $27.9B, up 15%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Net Income: $15.0B, up 10%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Diluted EPS: $2.95, up 10%\n",
       "</code></pre></div>\n",
       "<ul>\n",
       "<li><strong>Proof:</strong> <em>Not fully supported. The FY24 Q4 earnings release provided in the context (Microsoft) reports quarterly net income of $22.0 billion for the quarter ended June 30, 2024, not $15.0 billion. Therefore, the claim contradicts the document.</em></li>\n",
       "</ul>\n",
       "<hr />\n",
       "<span class=\"paragraph\"><strong>Claim 6:</strong> <em>The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company is not identified] reports that net income was up 10% [relative to an unspecified prior period].</em></span>\n",
       "<ul>\n",
       "<li><strong>Status:</strong> ✅ Supported</li>\n",
       "<li><strong>Original Sentence:</strong></li>\n",
       "</ul>\n",
       "<div class=\"language-markdown codehilite\"><pre><span></span><code> Here are the key metrics from the FY24 Q4 earnings release:\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Quarterly Revenue: $64.7B, up 15%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Operating Income: $27.9B, up 15%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Net Income: $15.0B, up 10%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Diluted EPS: $2.95, up 10%\n",
       "</code></pre></div>\n",
       "<ul>\n",
       "<li><strong>Proof:</strong> <em>Supported. The loaded document is titled “Earnings Release FY24 Q4,” and within it the company states: “Net income was $22.0 billion and increased 10% (up 11% in constant currency).” This directly supports that the FY24 Q4 earnings release reports net income was up 10%.</em></li>\n",
       "</ul>\n",
       "<hr />\n",
       "<span class=\"paragraph\"><strong>Claim 7:</strong> <em>The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company is not identified] reports diluted EPS of $2.95.</em></span>\n",
       "<ul>\n",
       "<li><strong>Status:</strong> ✅ Supported</li>\n",
       "<li><strong>Original Sentence:</strong></li>\n",
       "</ul>\n",
       "<div class=\"language-markdown codehilite\"><pre><span></span><code> Here are the key metrics from the FY24 Q4 earnings release:\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Quarterly Revenue: $64.7B, up 15%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Operating Income: $27.9B, up 15%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Net Income: $15.0B, up 10%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Diluted EPS: $2.95, up 10%\n",
       "</code></pre></div>\n",
       "<ul>\n",
       "<li><strong>Proof:</strong> <em>The loaded document titled “Earnings Release FY24 Q4” explicitly states under results for the quarter ended June 30, 2024: “Diluted earnings per share was $2.95,” directly supporting that the FY24 Q4 earnings release reports diluted EPS of $2.95.</em></li>\n",
       "</ul>\n",
       "<hr />\n",
       "<span class=\"paragraph\"><strong>Claim 8:</strong> <em>The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company is not identified] reports that diluted EPS was up 10% [relative to an unspecified prior period].</em></span>\n",
       "<ul>\n",
       "<li><strong>Status:</strong> ✅ Supported</li>\n",
       "<li><strong>Original Sentence:</strong></li>\n",
       "</ul>\n",
       "<div class=\"language-markdown codehilite\"><pre><span></span><code> Here are the key metrics from the FY24 Q4 earnings release:\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Quarterly Revenue: $64.7B, up 15%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Operating Income: $27.9B, up 15%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Net Income: $15.0B, up 10%\n",
       "<span class=\"k\">-</span><span class=\"w\"> </span>Diluted EPS: $2.95, up 10%\n",
       "</code></pre></div>\n",
       "<ul>\n",
       "<li><strong>Proof:</strong> <em>Supported. The FY24 Q4 earnings release states: “Diluted earnings per share was $2.95 and increased 10% (up 11% in constant currency),” which directly reports that diluted EPS was up 10%. The document header also indicates it is the “Earnings Release FY24 Q4.”</em></li>\n",
       "</ul></span>"
      ],
      "text/markdown": [
       "## Claim Verification Results\n",
       "\n",
       "- **Is the evaluation applicable to this example? *yes***\n",
       "- **Overall Score: *87.50%***\n",
       "\n",
       "### Claim Statistics\n",
       "\n",
       "- **Total Claims:** *8*\n",
       "- **Supported Claims:** *7*\n",
       "- **Open Domain Claims:** *0*\n",
       "- **Unsupported Claims:** *1*\n",
       "\n",
       "---\n",
       "\n",
       "### Detailed Claim Analysis\n",
       "\n",
       "*NOTE: Citations are not included in this output, see the dedicated notebook for more details*\n",
       "\n",
       "**Claim 1:** *The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company is not identified] reports revenue of $64.7 billion.*\n",
       "\n",
       "- **Status:** ✅ Supported\n",
       "- **Original Sentence:**\n",
       "\n",
       "```markdown\n",
       " Here are the key metrics from the FY24 Q4 earnings release:\n",
       "- Quarterly Revenue: $64.7B, up 15%\n",
       "- Operating Income: $27.9B, up 15%\n",
       "- Net Income: $15.0B, up 10%\n",
       "- Diluted EPS: $2.95, up 10%\n",
       "```\n",
       "\n",
       "- **Proof:** *The FY24 Q4 earnings release file explicitly states for the quarter ended June 30, 2024: “Revenue was $64.7 billion and increased 15% (up 16% in constant currency).” This directly supports that the earnings release reports revenue of $64.7 billion.*\n",
       "\n",
       "---\n",
       "**Claim 2:** *The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company is not identified] reports that revenue was up 15% [relative to an unspecified prior period].*\n",
       "\n",
       "- **Status:** ✅ Supported\n",
       "- **Original Sentence:**\n",
       "\n",
       "```markdown\n",
       " Here are the key metrics from the FY24 Q4 earnings release:\n",
       "- Quarterly Revenue: $64.7B, up 15%\n",
       "- Operating Income: $27.9B, up 15%\n",
       "- Net Income: $15.0B, up 10%\n",
       "- Diluted EPS: $2.95, up 10%\n",
       "```\n",
       "\n",
       "- **Proof:** *Supported. The FY24 Q4 earnings release text states: “Revenue was $64.7 billion and increased 15% (up 16% in constant currency)” in the section “for the quarter ended June 30, 2024, as compared to the corresponding period of last fiscal year,” and the file is titled “Earnings Release FY24 Q4.” This directly supports that the FY24 Q4 earnings release reports revenue up 15%.*\n",
       "\n",
       "---\n",
       "**Claim 3:** *The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company is not identified] reports operating income of $27.9 billion.*\n",
       "\n",
       "- **Status:** ✅ Supported\n",
       "- **Original Sentence:**\n",
       "\n",
       "```markdown\n",
       " Here are the key metrics from the FY24 Q4 earnings release:\n",
       "- Quarterly Revenue: $64.7B, up 15%\n",
       "- Operating Income: $27.9B, up 15%\n",
       "- Net Income: $15.0B, up 10%\n",
       "- Diluted EPS: $2.95, up 10%\n",
       "```\n",
       "\n",
       "- **Proof:** *The FY24 Q4 earnings release text explicitly states: “Operating income was $27.9 billion and increased 15% (up 16% in constant currency),” confirming that operating income reported for the quarter was $27.9 billion. The same document header identifies it as “Earnings Release FY24 Q4.”*\n",
       "\n",
       "---\n",
       "**Claim 4:** *The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company is not identified] reports that operating income was up 15% [relative to an unspecified prior period].*\n",
       "\n",
       "- **Status:** ✅ Supported\n",
       "- **Original Sentence:**\n",
       "\n",
       "```markdown\n",
       " Here are the key metrics from the FY24 Q4 earnings release:\n",
       "- Quarterly Revenue: $64.7B, up 15%\n",
       "- Operating Income: $27.9B, up 15%\n",
       "- Net Income: $15.0B, up 10%\n",
       "- Diluted EPS: $2.95, up 10%\n",
       "```\n",
       "\n",
       "- **Proof:** *The loaded file list shows a document named “Earnings Release FY24 Q4.md,” indicating the FY24 Q4 earnings release. In the file content, it states: “Operating income was $27.9 billion and increased 15% (up 16% in constant currency)” for the quarter ended June 30, 2024, compared to the corresponding period of last fiscal year. This supports the claim that the FY24 Q4 earnings release reports operating income was up 15%.*\n",
       "\n",
       "---\n",
       "**Claim 5:** *The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company is not identified] reports net income of $15.0 billion.*\n",
       "\n",
       "- **Status:** ❌ Unsupported\n",
       "- **Original Sentence:**\n",
       "\n",
       "```markdown\n",
       " Here are the key metrics from the FY24 Q4 earnings release:\n",
       "- Quarterly Revenue: $64.7B, up 15%\n",
       "- Operating Income: $27.9B, up 15%\n",
       "- Net Income: $15.0B, up 10%\n",
       "- Diluted EPS: $2.95, up 10%\n",
       "```\n",
       "\n",
       "- **Proof:** *Not fully supported. The FY24 Q4 earnings release provided in the context (Microsoft) reports quarterly net income of $22.0 billion for the quarter ended June 30, 2024, not $15.0 billion. Therefore, the claim contradicts the document.*\n",
       "\n",
       "---\n",
       "**Claim 6:** *The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company is not identified] reports that net income was up 10% [relative to an unspecified prior period].*\n",
       "\n",
       "- **Status:** ✅ Supported\n",
       "- **Original Sentence:**\n",
       "\n",
       "```markdown\n",
       " Here are the key metrics from the FY24 Q4 earnings release:\n",
       "- Quarterly Revenue: $64.7B, up 15%\n",
       "- Operating Income: $27.9B, up 15%\n",
       "- Net Income: $15.0B, up 10%\n",
       "- Diluted EPS: $2.95, up 10%\n",
       "```\n",
       "\n",
       "- **Proof:** *Supported. The loaded document is titled “Earnings Release FY24 Q4,” and within it the company states: “Net income was $22.0 billion and increased 10% (up 11% in constant currency).” This directly supports that the FY24 Q4 earnings release reports net income was up 10%.*\n",
       "\n",
       "---\n",
       "**Claim 7:** *The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company is not identified] reports diluted EPS of $2.95.*\n",
       "\n",
       "- **Status:** ✅ Supported\n",
       "- **Original Sentence:**\n",
       "\n",
       "```markdown\n",
       " Here are the key metrics from the FY24 Q4 earnings release:\n",
       "- Quarterly Revenue: $64.7B, up 15%\n",
       "- Operating Income: $27.9B, up 15%\n",
       "- Net Income: $15.0B, up 10%\n",
       "- Diluted EPS: $2.95, up 10%\n",
       "```\n",
       "\n",
       "- **Proof:** *The loaded document titled “Earnings Release FY24 Q4” explicitly states under results for the quarter ended June 30, 2024: “Diluted earnings per share was $2.95,” directly supporting that the FY24 Q4 earnings release reports diluted EPS of $2.95.*\n",
       "\n",
       "---\n",
       "**Claim 8:** *The FY24 Q4 earnings release [document: an earnings release labeled \"FY24 Q4\"; the specific company is not identified] reports that diluted EPS was up 10% [relative to an unspecified prior period].*\n",
       "\n",
       "- **Status:** ✅ Supported\n",
       "- **Original Sentence:**\n",
       "\n",
       "```markdown\n",
       " Here are the key metrics from the FY24 Q4 earnings release:\n",
       "- Quarterly Revenue: $64.7B, up 15%\n",
       "- Operating Income: $27.9B, up 15%\n",
       "- Net Income: $15.0B, up 10%\n",
       "- Diluted EPS: $2.95, up 10%\n",
       "```\n",
       "\n",
       "- **Proof:** *Supported. The FY24 Q4 earnings release states: “Diluted earnings per share was $2.95 and increased 10% (up 11% in constant currency),” which directly reports that diluted EPS was up 10%. The document header also indicates it is the “Earnings Release FY24 Q4.”*"
      ],
      "text/plain": [
       "_md()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_result = get_result_by_name(results, \"claim_verification\")\n",
    "\n",
    "# Extract claim verification metadata\n",
    "cv_metadata = cv_result.metadata\n",
    "cv_total_claims = cv_metadata.get(\"total_claims\", 0)\n",
    "cv_supported = cv_metadata.get(\"number_supported_claims\", 0)\n",
    "cv_open_domain = cv_metadata.get(\"number_open_domain_claims\", 0)\n",
    "cv_not_supported = cv_metadata.get(\"number_not_supported_claims\", 0)\n",
    "cv_claims = cv_metadata.get(\"claims\", [])\n",
    "\n",
    "# Build claim statistics section\n",
    "cv_stats_section = \"\"\n",
    "if cv_total_claims > 0:\n",
    "    cv_stats_section = f\"\"\"\n",
    "### Claim Statistics\n",
    "\n",
    "- **Total Claims:** *{cv_total_claims}*\n",
    "- **Supported Claims:** *{cv_supported}*\n",
    "- **Open Domain Claims:** *{cv_open_domain}*\n",
    "- **Unsupported Claims:** *{cv_not_supported}*\"\"\"\n",
    "\n",
    "# Build individual claims section\n",
    "cv_claims_sections = []\n",
    "for idx_cv, claim in enumerate(cv_claims):\n",
    "    # Determine verification status\n",
    "    verification_status = \"❌ Unsupported\"\n",
    "    if claim.get(\"citations\", []):\n",
    "        verification_status = \"✅ Supported\"\n",
    "    elif claim.get(\"is_open_domain\", False):\n",
    "        verification_status = \"🌐 Open Domain\"\n",
    "\n",
    "    # Build citations text\n",
    "    citations_text = \"\"\n",
    "    if claim.get(\"citations\", []):\n",
    "        citation_items = []\n",
    "        for citation in claim[\"citations\"]:\n",
    "            citation_items.append(\n",
    "                f\"Source {citation['source_id']}: *{citation.get('cited_text', 'N/A')}*\"\n",
    "            )\n",
    "        citations_text = \"\\n    - \" + \"\\n    - \".join(citation_items)\n",
    "\n",
    "    cv_claims_sections.append(f\"\"\"**Claim {idx_cv + 1}:** *{claim.get(\"claim\", \"N/A\")}*\n",
    "\n",
    "- **Status:** {verification_status}\n",
    "- **Original Sentence:**\n",
    "\n",
    "```markdown\n",
    " {claim.get(\"sentence\", \"N/A\")}\n",
    "```\n",
    "\n",
    "- **Proof:** *{claim.get(\"proof\", \"N/A\") if claim.get(\"proof\") else \"No proof provided\"}*\n",
    "{f\"- **Open Domain Justification:** *{claim.get('open_domain_justification', '')}*\" if claim.get(\"is_open_domain\") else \"\"}\"\"\")\n",
    "\n",
    "cv_claims_md = (\n",
    "    \"\\n---\\n\".join(cv_claims_sections)\n",
    "    if cv_claims_sections\n",
    "    else \"*No claims evaluated.*\"\n",
    ")\n",
    "\n",
    "# Build the final markdown\n",
    "cv_details = \"\"\n",
    "if cv_result.applicable and cv_total_claims > 0:\n",
    "    cv_details = f\"\"\"{cv_stats_section}\n",
    "\n",
    "---\n",
    "\n",
    "### Detailed Claim Analysis\n",
    "\n",
    "*NOTE: Citations are not included in this output, see the dedicated notebook for more details*\n",
    "\n",
    "{cv_claims_md}\"\"\"\n",
    "\n",
    "mo.md(\n",
    "    render_evaluation_result(\n",
    "        \"Claim Verification Results\",\n",
    "        cv_result,\n",
    "        custom_details=cv_details,\n",
    "        no_data_message=\"No claims to verify or evaluation not applicable.\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ZHCJ",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T20:12:50.248181Z",
     "iopub.status.busy": "2025-08-22T20:12:50.248090Z",
     "iopub.status.idle": "2025-08-22T20:12:50.256857Z",
     "shell.execute_reply": "2025-08-22T20:12:50.256572Z"
    },
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"markdown prose dark:prose-invert\"><h2 id=\"tool-usage-results\">Tool Usage Results</h2>\n",
       "<ul>\n",
       "<li><strong>Is the evaluation applicable to this example? <em>yes</em></strong></li>\n",
       "<li><strong>Overall Score: <em>100.00%</em></strong></li>\n",
       "</ul>\n",
       "<h3 id=\"tool-call-analysis\">Tool Call Analysis</h3>\n",
       "<span class=\"paragraph\">The evaluation determines the probability that each tool should have been called based on the conversation context.</span>\n",
       "<ul>\n",
       "<li>If no tools should be called (was_called=False for all)<ul>\n",
       "<li>Each probability should be below the threshold. If so, return 100, 0 otherwise.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li>If was_called=True for any tool:<ul>\n",
       "<li>Check any of those tool's probability is above its threshold, if so return 100.\n",
       "Otherwise if none of the \"was_called=True\" have a probability over the threshold,  return 0.</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ul>\n",
       "<span class=\"paragraph\"><strong>Tool 1: <em>search</em></strong></span>\n",
       "<ul>\n",
       "<li><strong>Probability Should Be Called:</strong> 🔴 <em>8.0%</em></li>\n",
       "<li><strong>Reasoning:</strong> <em>For: If the \"earnings release file\" is not in the workspace or the user actually meant a public company press release, a quick web search could locate it or confirm context. Against: The user explicitly asked to load a file, implying it exists locally. Using search risks fetching the wrong document and adds latency; local file inspection is more appropriate. Given the low search-depth constraint and preference for quick answers, search is unlikely unless the file cannot be found locally.</em></li>\n",
       "</ul>\n",
       "<hr />\n",
       "<span class=\"paragraph\"><strong>Tool 2: <em>edit_file</em></strong></span>\n",
       "<ul>\n",
       "<li><strong>Probability Should Be Called:</strong> 🔴 <em>3.0%</em></li>\n",
       "<li><strong>Reasoning:</strong> <em>For: Could be used to create/save a summarized document of key metrics for the user. Against: The user asked only to tell them the key metrics, not to create or modify files. Editing/creating a file would be extra and unnecessary for the stated goal.</em></li>\n",
       "</ul>\n",
       "<hr />\n",
       "<span class=\"paragraph\"><strong>Tool 3: <em>ls</em></strong></span>\n",
       "<ul>\n",
       "<li><strong>Probability Should Be Called:</strong> 🟢 <em>78.0%</em></li>\n",
       "<li><strong>Reasoning:</strong> <em>For: We likely need to locate the earnings release file since no path/name was provided. Listing the workspace (and possibly a common docs directory) is the safest first step within the two-call budget. Against: If the assistant already knows or infers a standard path/name and opts to directly view it to save a call, ls might be skipped. There’s moderate uncertainty if a direct view attempt is made first to stay within minimal calls.</em></li>\n",
       "</ul>\n",
       "<hr />\n",
       "<span class=\"paragraph\"><strong>Tool 4: <em>view</em></strong></span>\n",
       "<ul>\n",
       "<li><strong>Probability Should Be Called:</strong> 🟢 <em>92.0%</em></li>\n",
       "<li><strong>Reasoning:</strong> <em>For: To extract key metrics, we need to read the contents of the earnings release file; viewing it is essential. This is the most direct way to fulfill the request once a path is known. Against: If the file is binary or not supported by the viewer, or if it cannot be found, the call may fail. Still, given the request, attempting to view is highly likely.</em></li>\n",
       "</ul></span>"
      ],
      "text/markdown": [
       "## Tool Usage Results\n",
       "\n",
       "- **Is the evaluation applicable to this example? *yes***\n",
       "- **Overall Score: *100.00%***\n",
       "\n",
       "### Tool Call Analysis\n",
       "\n",
       "The evaluation determines the probability that each tool should have been called based on the conversation context.\n",
       "\n",
       "- If no tools should be called (was_called=False for all)\n",
       "    - Each probability should be below the threshold. If so, return 100, 0 otherwise.\n",
       "- If was_called=True for any tool:\n",
       "    - Check any of those tool's probability is above its threshold, if so return 100.\n",
       "    Otherwise if none of the \"was_called=True\" have a probability over the threshold,  return 0.\n",
       "\n",
       "\n",
       "**Tool 1: *search***\n",
       "\n",
       "- **Probability Should Be Called:** 🔴 *8.0%*\n",
       "- **Reasoning:** *For: If the \"earnings release file\" is not in the workspace or the user actually meant a public company press release, a quick web search could locate it or confirm context. Against: The user explicitly asked to load a file, implying it exists locally. Using search risks fetching the wrong document and adds latency; local file inspection is more appropriate. Given the low search-depth constraint and preference for quick answers, search is unlikely unless the file cannot be found locally.*\n",
       "---\n",
       "\n",
       "**Tool 2: *edit_file***\n",
       "\n",
       "- **Probability Should Be Called:** 🔴 *3.0%*\n",
       "- **Reasoning:** *For: Could be used to create/save a summarized document of key metrics for the user. Against: The user asked only to tell them the key metrics, not to create or modify files. Editing/creating a file would be extra and unnecessary for the stated goal.*\n",
       "---\n",
       "\n",
       "**Tool 3: *ls***\n",
       "\n",
       "- **Probability Should Be Called:** 🟢 *78.0%*\n",
       "- **Reasoning:** *For: We likely need to locate the earnings release file since no path/name was provided. Listing the workspace (and possibly a common docs directory) is the safest first step within the two-call budget. Against: If the assistant already knows or infers a standard path/name and opts to directly view it to save a call, ls might be skipped. There’s moderate uncertainty if a direct view attempt is made first to stay within minimal calls.*\n",
       "---\n",
       "\n",
       "**Tool 4: *view***\n",
       "\n",
       "- **Probability Should Be Called:** 🟢 *92.0%*\n",
       "- **Reasoning:** *For: To extract key metrics, we need to read the contents of the earnings release file; viewing it is essential. This is the most direct way to fulfill the request once a path is known. Against: If the file is binary or not supported by the viewer, or if it cannot be found, the call may fail. Still, given the request, attempting to view is highly likely.*"
      ],
      "text/plain": [
       "_md()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu_result = get_result_by_name(results, \"tool_usage\")\n",
    "\n",
    "# Extract tool usage metadata\n",
    "tu_metadata = tu_result.metadata\n",
    "tu_evaluations = tu_metadata.get(\"tool_evaluations\", {})\n",
    "tu_tool_list = (\n",
    "    tu_evaluations.get(\"tool_evaluations\", [])\n",
    "    if isinstance(tu_evaluations, dict)\n",
    "    else []\n",
    ")\n",
    "\n",
    "# Build individual tool evaluations section\n",
    "tu_tool_sections = []\n",
    "for idx_tu, tool_eval in enumerate(tu_tool_list):\n",
    "    tool_name = tool_eval.get(\"tool_name\", \"N/A\")\n",
    "    tool_reasoning = tool_eval.get(\"reasoning\", \"N/A\")\n",
    "    tool_probability = tool_eval.get(\"probability\", 0)\n",
    "\n",
    "    # Determine if this tool probability indicates it should have been called\n",
    "    probability_indicator = \"🟢\" if tool_probability >= 50 else \"🔴\"\n",
    "\n",
    "    tu_tool_sections.append(f\"\"\"\n",
    "**Tool {idx_tu + 1}: *{tool_name}***\n",
    "\n",
    "- **Probability Should Be Called:** {probability_indicator} *{tool_probability:.1f}%*\n",
    "- **Reasoning:** *{tool_reasoning}*\"\"\")\n",
    "\n",
    "tu_tools_md = (\n",
    "    \"\\n---\\n\".join(tu_tool_sections)\n",
    "    if tu_tool_sections\n",
    "    else \"*No tools evaluated.*\"\n",
    ")\n",
    "\n",
    "# Build the final markdown\n",
    "tu_details = \"\"\n",
    "if tu_result.applicable and tu_tool_list:\n",
    "    tu_details = f\"\"\"\n",
    "### Tool Call Analysis\n",
    "\n",
    "The evaluation determines the probability that each tool should have been called based on the conversation context.\n",
    "\n",
    "- If no tools should be called (was_called=False for all)\n",
    "    - Each probability should be below the threshold. If so, return 100, 0 otherwise.\n",
    "- If was_called=True for any tool:\n",
    "    - Check any of those tool's probability is above its threshold, if so return 100.\n",
    "    Otherwise if none of the \"was_called=True\" have a probability over the threshold,  return 0.\n",
    "\n",
    "{tu_tools_md}\"\"\"\n",
    "\n",
    "mo.md(\n",
    "    render_evaluation_result(\n",
    "        \"Tool Usage Results\",\n",
    "        tu_result,\n",
    "        custom_details=tu_details,\n",
    "        no_data_message=\"No tools to evaluate or evaluation not applicable.\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ROlb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T20:12:50.257667Z",
     "iopub.status.busy": "2025-08-22T20:12:50.257577Z",
     "iopub.status.idle": "2025-08-22T20:12:50.404800Z",
     "shell.execute_reply": "2025-08-22T20:12:50.404078Z"
    },
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"markdown prose dark:prose-invert\"><h1 id=\"feedback\">Feedback</h1>\n",
       "<span class=\"paragraph\">Each evaluation will optionally output <code>feedback</code>, which can be used to get more details about why a (typically bad) score was given. Below is the feedback for this scenario.</span>\n",
       "<h2 id=\"generated-feedback\">Generated Feedback</h2>\n",
       "<span class=\"paragraph\"><strong>preference_adherence</strong>:\n",
       "<div class=\"language-xml codehilite\"><pre><span></span><code>The<span class=\"w\"> </span>following<span class=\"w\"> </span>preferences<span class=\"w\"> </span>were<span class=\"w\"> </span>extracted<span class=\"w\"> </span>as<span class=\"w\"> </span>aspects<span class=\"w\"> </span>the<span class=\"w\"> </span>user<span class=\"w\"> </span>wanted,<span class=\"w\"> </span>but\n",
       "the<span class=\"w\"> </span>assistant<span class=\"w\"> </span>did<span class=\"w\"> </span>not<span class=\"w\"> </span>adhere<span class=\"w\"> </span>to<span class=\"w\"> </span>them.\n",
       "<span class=\"nt\">&lt;preference&gt;</span>The<span class=\"w\"> </span>user<span class=\"w\"> </span>prefers<span class=\"w\"> </span>responses<span class=\"w\"> </span>in<span class=\"w\"> </span>paragraph<span class=\"w\"> </span>form<span class=\"nt\">&lt;/preference&gt;</span>\n",
       "<span class=\"nt\">&lt;reasoning&gt;</span>The<span class=\"w\"> </span>assistant’s<span class=\"w\"> </span>last<span class=\"w\"> </span>response<span class=\"w\"> </span>presented<span class=\"w\"> </span>the<span class=\"w\"> </span>key<span class=\"w\"> </span>metrics<span class=\"w\"> </span>as<span class=\"w\"> </span>a<span class=\"w\"> </span>bulleted<span class=\"w\"> </span>list<span class=\"w\"> </span>rather<span class=\"w\"> </span>than<span class=\"w\"> </span>a<span class=\"w\"> </span>paragraph.<span class=\"w\"> </span>Since<span class=\"w\"> </span>the<span class=\"w\"> </span>stored<span class=\"w\"> </span>user<span class=\"w\"> </span>preference<span class=\"w\"> </span>explicitly<span class=\"w\"> </span>requests<span class=\"w\"> </span>paragraph<span class=\"w\"> </span>form,<span class=\"w\"> </span>and<span class=\"w\"> </span>summarizing<span class=\"w\"> </span>metrics<span class=\"w\"> </span>can<span class=\"w\"> </span>be<span class=\"w\"> </span>delivered<span class=\"w\"> </span>in<span class=\"w\"> </span>a<span class=\"w\"> </span>paragraph,<span class=\"w\"> </span>the<span class=\"w\"> </span>response<span class=\"w\"> </span>did<span class=\"w\"> </span>not<span class=\"w\"> </span>follow<span class=\"w\"> </span>the<span class=\"w\"> </span>preference.<span class=\"w\"> </span>There<span class=\"w\"> </span>is<span class=\"w\"> </span>no<span class=\"w\"> </span>contextual<span class=\"w\"> </span>reason<span class=\"w\"> </span>making<span class=\"w\"> </span>the<span class=\"w\"> </span>preference<span class=\"w\"> </span>inapplicable<span class=\"w\"> </span>here.<span class=\"nt\">&lt;/reasoning&gt;</span>\n",
       "</code></pre></div></span>\n",
       "<span class=\"paragraph\"><strong>claim_verification</strong>:\n",
       "<div class=\"language-xml codehilite\"><pre><span></span><code>The<span class=\"w\"> </span>following<span class=\"w\"> </span>sentences<span class=\"w\"> </span>were<span class=\"w\"> </span>determined<span class=\"w\"> </span>to<span class=\"w\"> </span>possibly<span class=\"w\"> </span>have<span class=\"w\"> </span>portions<span class=\"w\"> </span>of<span class=\"w\"> </span>them<span class=\"w\"> </span>that<span class=\"w\"> </span>have<span class=\"w\"> </span>unverified<span class=\"w\"> </span>content<span class=\"w\"> </span>in<span class=\"w\"> </span>them<span class=\"w\"> </span>and<span class=\"w\"> </span>reasoning<span class=\"w\"> </span>provides<span class=\"w\"> </span>the<span class=\"w\"> </span>justification<span class=\"w\"> </span>for<span class=\"w\"> </span>that.\n",
       "<span class=\"nt\">&lt;sentence&gt;</span>Here<span class=\"w\"> </span>are<span class=\"w\"> </span>the<span class=\"w\"> </span>key<span class=\"w\"> </span>metrics<span class=\"w\"> </span>from<span class=\"w\"> </span>the<span class=\"w\"> </span>FY24<span class=\"w\"> </span>Q4<span class=\"w\"> </span>earnings<span class=\"w\"> </span>release:\n",
       "-<span class=\"w\"> </span>Quarterly<span class=\"w\"> </span>Revenue:<span class=\"w\"> </span>$64.7B,<span class=\"w\"> </span>up<span class=\"w\"> </span>15%\n",
       "-<span class=\"w\"> </span>Operating<span class=\"w\"> </span>Income:<span class=\"w\"> </span>$27.9B,<span class=\"w\"> </span>up<span class=\"w\"> </span>15%\n",
       "-<span class=\"w\"> </span>Net<span class=\"w\"> </span>Income:<span class=\"w\"> </span>$15.0B,<span class=\"w\"> </span>up<span class=\"w\"> </span>10%\n",
       "-<span class=\"w\"> </span>Diluted<span class=\"w\"> </span>EPS:<span class=\"w\"> </span>$2.95,<span class=\"w\"> </span>up<span class=\"w\"> </span>10%<span class=\"nt\">&lt;/sentence&gt;</span>\n",
       "<span class=\"nt\">&lt;unverified_reasoning&gt;</span>Not<span class=\"w\"> </span>fully<span class=\"w\"> </span>supported.<span class=\"w\"> </span>The<span class=\"w\"> </span>FY24<span class=\"w\"> </span>Q4<span class=\"w\"> </span>earnings<span class=\"w\"> </span>release<span class=\"w\"> </span>provided<span class=\"w\"> </span>in<span class=\"w\"> </span>the<span class=\"w\"> </span>context<span class=\"w\"> </span>(Microsoft)<span class=\"w\"> </span>reports<span class=\"w\"> </span>quarterly<span class=\"w\"> </span>net<span class=\"w\"> </span>income<span class=\"w\"> </span>of<span class=\"w\"> </span>$22.0<span class=\"w\"> </span>billion<span class=\"w\"> </span>for<span class=\"w\"> </span>the<span class=\"w\"> </span>quarter<span class=\"w\"> </span>ended<span class=\"w\"> </span>June<span class=\"w\"> </span>30,<span class=\"w\"> </span>2024,<span class=\"w\"> </span>not<span class=\"w\"> </span>$15.0<span class=\"w\"> </span>billion.<span class=\"w\"> </span>Therefore,<span class=\"w\"> </span>the<span class=\"w\"> </span>claim<span class=\"w\"> </span>contradicts<span class=\"w\"> </span>the<span class=\"w\"> </span>document.<span class=\"nt\">&lt;/unverified_reasoning&gt;</span>\n",
       "</code></pre></div></span></span>"
      ],
      "text/markdown": [
       "# Feedback\n",
       "\n",
       "Each evaluation will optionally output `feedback`, which can be used to get more details about why a (typically bad) score was given. Below is the feedback for this scenario.\n",
       "\n",
       "## Generated Feedback\n",
       "**preference_adherence**:\n",
       "```plaintext\n",
       "The following preferences were extracted as aspects the user wanted, but\n",
       "the assistant did not adhere to them.\n",
       "<preference>The user prefers responses in paragraph form</preference>\n",
       "<reasoning>The assistant’s last response presented the key metrics as a bulleted list rather than a paragraph. Since the stored user preference explicitly requests paragraph form, and summarizing metrics can be delivered in a paragraph, the response did not follow the preference. There is no contextual reason making the preference inapplicable here.</reasoning>\n",
       "```\n",
       "\n",
       "**claim_verification**:\n",
       "```plaintext\n",
       "The following sentences were determined to possibly have portions of them that have unverified content in them and reasoning provides the justification for that.\n",
       "<sentence>Here are the key metrics from the FY24 Q4 earnings release:\n",
       "- Quarterly Revenue: $64.7B, up 15%\n",
       "- Operating Income: $27.9B, up 15%\n",
       "- Net Income: $15.0B, up 10%\n",
       "- Diluted EPS: $2.95, up 10%</sentence>\n",
       "<unverified_reasoning>Not fully supported. The FY24 Q4 earnings release provided in the context (Microsoft) reports quarterly net income of $22.0 billion for the quarter ended June 30, 2024, not $15.0 billion. Therefore, the claim contradicts the document.</unverified_reasoning>\n",
       "```"
      ],
      "text/plain": [
       "_md()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback_str = \"\"\n",
    "for f_result in results:\n",
    "    if f_result.feedback:\n",
    "        feedback_str += (\n",
    "            f\"**{f_result.eval_name}**:\\n```plaintext\\n{f_result.feedback}\\n```\\n\\n\"\n",
    "        )\n",
    "feedback_str.strip()\n",
    "\n",
    "mo.md(f\"\"\"# Feedback\n",
    "\n",
    "Each evaluation will optionally output `feedback`, which can be used to get more details about why a (typically bad) score was given. Below is the feedback for this scenario.\n",
    "\n",
    "## Generated Feedback\n",
    "{feedback_str}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qnkX",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Next Steps\n",
    "\n",
    "Check out the low level APIs!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
