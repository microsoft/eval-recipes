# Eval Recipes

> **Eval Recipes** is a an evaluation framework that makes it easy to evaluate LLM chat assistants, and does so at a high degree of quality. We use recipes to develop specialized evaluation workflows that score conversations on dimensions such as how well the assistant adhered to user preferences or if they did not generate any hallucinations.

## Attribution Instructions

The built-in `claim_verification` evaluation is based on these two papers: [Claimify](https://arxiv.org/abs/2502.10855) and [VeriTrail](https://arxiv.org/abs/2505.21786). This is not an official implementation of either. Whenever you use the evaluation, please cite the original papers.


## README.md

<h1 align="center">
    Eval Recipes
</h1>
<p align="center">
    <p align="center">Evaluate AI agents with benchmarking harnesses and online evaluation recipes.
    </p>
</p>
<p align="center">
    <a href="https://github.com/astral-sh/uv"><img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/uv/main/assets/badge/v0.json" alt="uv"></a>
    <a href="https://www.python.org/"><img src="https://img.shields.io/badge/python-3.11+-blue.svg" alt="Python 3.11+"></a>
    <a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License: MIT"></a>
</p>

Eval Recipes is a library dedicated to make it easier to keep up with the state-of-the-art in evaluating AI agents.
It currently has two main components: a **benchmarking** harness for evaluating CLI agents (GitHub Copilot CLI, Claude Code, etc) on real-world tasks via containers and an **online evaluation** framework for LLM chat assistants.
The common thread between these components is the concept of [recipes](https://sundaylettersfromsam.substack.com/p/what-is-an-ai-recipe)
which are a mix of code and LLM calls to achieve a desired tradeoff between flexibility and quality.


## Installation

```bash
# Benchmarking requires certain prerequisites, see the full documentation for more details.
# With uv (add to project dependencies, pinned to a release tag)
uv add "eval-recipes @ git+https://github.com/microsoft/eval-recipes@v0.0.31"

# With pip
pip install "git+https://github.com/microsoft/eval-recipes@v0.0.31"
```

> [!WARNING]
> This library is very early and everything is subject to change. Consider pinning the dependency to a commit with the command like: `uv pip install "git+https://github.com/microsoft/eval-recipes@v0.0.20"`


# Benchmarking

Eval Recipes provides a benchmarking harness for evaluating AI agents on real-world tasks in isolated Docker containers. It supports score based, comparison based, and third party benchmarks.
We include tasks ranging from creating CLI applications to automations. Agents are automatically scored based on deterministic and semantic tests using a specialized auditing agent.
Additional features include agent continuation (automatically providing follow-up prompts when needed), multi-trial evaluation for consistency measurement, and reporting with HTML dashboards.

## Usage

1. Create agent definition(s). Examples are provided in [data/agents](./data/agents).
1. Create task definition(s). Examples are provided in [data/tasks](./data/tasks).
1. Create a run configuration. Examples are provided in [data/eval-setups](./data/eval-setups).

```python
import yaml
from eval_recipes.benchmarking.harness import Harness
from eval_recipes.benchmarking.schemas import ScoreRunSpec

with Path("score-default.yaml").open(encoding="utf-8") as f:
    run_definition = ScoreRunSpec(**yaml.safe_load(f))

harness = Harness(
    agents_dir=Path("data/agents"),
    tasks_dir=Path("data/tasks"),
    run_definition=run_definition,
)
asyncio.run(harness.run())
```

See [docs/BENCHMARKING.md](./docs/BENCHMARKING.md) for full details, including installation prerequisites.


---


# Online Evaluations

![Eval Recipes Animation](demos/data/EvalRecipesAnimation.gif)

## Get Started Quick!

### 1. View notebooks directly on GitHub

Located in [demos/](./demos).

### 2. Run interactive notebooks with marimo

Run demo notebooks (the `.py` files located at [demos/](./demos)) with [`marimo`](https://docs.marimo.io/getting_started/installation/).
Follow the installation section below if you do not have `uv` installed or environment variables configured.

```bash
uv run marimo edit demos/1_evaluate.py
# Select Y to run in a sandboxed venv
```


## High Level API

The primary way of interacting with the package is the high-level API which takes in a list of messages
(defined by [OpenAI's responses API](https://platform.openai.com/docs/api-reference/responses/create#responses_create-input))
and a list of [custom tool definitions](https://platform.openai.com/docs/api-reference/responses/create#responses_create-tools) (built-in tools are not supported).

Each evaluation will output if it is deemed applicable to your input, an overall `score` from 0 to 100, and additional metadata specific to that evaluation.

Currently there are several built-in evaluations: `claim_verification`, `tool_usage`, `guidance`, and `preference_adherence`.
For more details on how these evaluations work, check the Low Level API section below.
Each evaluation can be additionally configured, such as selecting the LLM used. The full configurations are defined in [schemas.py](./eval_recipes/schemas.py).

`evaluate` will return a list of [`EvaluationOutput`](./eval_recipes/schemas.py) instances corresponding to each evaluation.


> [!TIP]
> All of the code examples in this readme can be pasted into a `.py` file and run as is!

```python
import asyncio
from openai.types.chat.chat_completion_tool_param import ChatCompletionToolParam
from openai.types.responses import EasyInputMessageParam, ResponseInputParam
from eval_recipes.evaluate import evaluate
from eval_recipes.evaluations.check_criteria.check_criteria_evaluator import CheckCriteriaEvaluatorConfig
from eval_recipes.schemas import BaseEvaluatorConfig

async def main() -> None:
    messages: ResponseInputParam = [
        EasyInputMessageParam(
            role="system", content="You are a helpful assistant with search and document editing capabilities."
        ),
        EasyInputMessageParam(
            role="user",
            content="What material has the best elasticity for sports equipment? Please keep your response concise.",
        ),
        EasyInputMessageParam(
            role="assistant",
            content="Polyurethane elastomers offer excellent elasticity with 85% energy return and high durability.",
        ),
    ]

    tools: list[ChatCompletionToolParam] = [
        ChatCompletionToolParam(
            type="function",
            function={
                "name": "search",
                "description": "Search for information",
                "parameters": {"type": "object", "properties": {"query": {"type": "string"}}, "required": ["query"]},
            },
        ),
    ]
    config_preference_adherence = BaseEvaluatorConfig(model="gpt-5-mini")  # Sample config
    check_criteria = CheckCriteriaEvaluatorConfig(criteria=["Your response should be at least one paragraph long."])
    result = await evaluate(
        messages=messages,
        tools=tools,
        evaluations=["check_criteria", "claim_verification", "guidance", "preference_adherence", "tool_usage"],
        evaluation_configs={"preference_adherence": config_preference_adherence, "check_criteria": check_criteria},
        max_concurrency=1,
    )
    print(result)

asyncio.run(main())
```


### Custom Evaluations

You can create custom evaluators by implementing a class that follows the [`EvaluatorProtocol`](./eval_recipes/schemas.py).
This allows you to extend the evaluation framework with domain-specific metrics tailored to your needs.

Custom evaluators must implement:
1. An `__init__` method that accepts an optional `BaseEvaluatorConfig` parameter. If a config is not provided, you must initialize a default.
2. An async `evaluate` method that takes messages and tools as input and returns an `EvaluationOutput`

Here is an example of a custom evaluator that scores based on the length of the assistant's response being used in conjunction with the `preference_adherence` evaluator:

```python
import asyncio
from openai.types.chat.chat_completion_tool_param import ChatCompletionToolParam
from openai.types.responses import EasyInputMessageParam, ResponseInputParam
from eval_recipes.evaluate import evaluate
from eval_recipes.schemas import BaseEvaluatorConfig, EvaluationOutput

class ResponseLengthEvaluator:
    """Custom evaluator that scores based on response brevity."""
    def __init__(self, config: BaseEvaluatorConfig | None = None) -> None:
        self.config = config or BaseEvaluatorConfig()

    async def evaluate(self, messages: ResponseInputParam, tools: list[ChatCompletionToolParam]) -> EvaluationOutput:
        total_length = 0
        for message in reversed(messages):  # Only look at the last assistant message
            if ("role" in message and message["role"] == "assistant") and message.get("content"):
                total_length += len(str(message["content"]))
                break

        score = max(0, 100 - int(total_length // 25))  # Decrease score as length increases
        return EvaluationOutput(eval_name="response_length", applicable=True, score=score, metadata={})

async def main() -> None:
    messages: ResponseInputParam = [
        EasyInputMessageParam(
            role="user",
            content="What material has the best elasticity for sports equipment? Please keep your response concise.",
        ),
        EasyInputMessageParam(
            role="assistant",
            content="Polyurethane elastomers offer excellent elasticity with 85% energy return and high durability.",
        ),
    ]
    result = await evaluate(
        messages=messages,
        tools=[],
        evaluations=[ResponseLengthEvaluator, "preference_adherence"],
        evaluation_configs={"ResponseLengthEvaluator": BaseEvaluatorConfig(model="gpt-5-mini")},
        max_concurrency=1,
    )
    print(result)

asyncio.run(main())
```


## Development Installation
### Prerequisites
- make
  - For Windows, you can download it using [UniGetUI](https://github.com/marticliment/UnigetUI) and use [ezwinports make](https://github.com/microsoft/winget-pkgs/tree/master/manifests/e/ezwinports/make)
- [uv](https://docs.astral.sh/uv/getting-started/installation/)

### Install Dependencies & Configure Environment

```bash
make install
cp .env.sample .env
# Configure API keys in .env
# Make sure the venv gets activated
. .venv/bin/activate # Linux example
```

This library requires either OpenAI or Azure OpenAI to be configured. You must set the correct environment variables in the `.env` file.

The `semantic_test` evaluator additionally requires `ANTHROPIC_API_KEY` to be set, as it uses the Claude Agent SDK.

Check [utils.py `create_client`](./eval_recipes/utils/llm.py) to troubleshoot any configuration issues.

### Other

- [Generating Jupyter Notebooks](./docs/NOTEBOOKS.md)
- To re-create the [Manim](https://www.manim.community/) animation:
  - `make install-all` to install manim. See the docs if you have issues on a Linux-based system. Note this will also require `ffmpeg` to be installed.
  - `uv run manim scripts/create_animation.py EvalRecipesAnimation -qh && ffmpeg -y -i media/videos/create_animation/1080p60/EvalRecipesAnimation.mp4 -vf "fps=30,scale=1920:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse" -loop 0 demos/data/EvalRecipesAnimation.gif`
- [Validating Evaluations](./tests/validate_evaluations.py):
  - This script will run evaluations against a small "goldset" (see [data/goldset](data/goldset/)) where we have inputs to evaluate with labels of what the scores should be (defined in [data/goldset/labels.yaml](data/goldset/labels.yaml)).


## Low Level API

[LOW_LEVEL_API.md](./docs/LOW_LEVEL_API.md)


## Changelog

[CHANGELOG.md](./docs/CHANGELOG.md)


## Roadmap

[ROADMAP.md](./docs/ROADMAP.md)


## Attributions

The built-in `claim_verification` evaluation is based on these two papers: [Claimify](https://arxiv.org/abs/2502.10855) and [VeriTrail](https://arxiv.org/abs/2505.21786). This is not an official implementation of either and please cite the original papers if you use this evaluation in your work.

## docs/LOW_LEVEL_API.md

## Low Level API

We provide the low level API because the high level API makes assumptions about the structure of
your messages, tools, and how they are used in the evaluation.
For example, claim verification automatically ignores any assistant messages from the data that is verified against.
The low level API for each evaluation typically allows for more granular control of how to handle the data you'd like to be evaluated.


### Check Criteria

Evaluates assistant responses against custom criteria or rubrics you define. This is a useful "catch-all" evaluation for simpler requirements like tone, format, or content guidelines.

**Metric**: Average probability across all criteria (0-100), where each criterion is evaluated independently

```python
import asyncio

from eval_recipes.evaluations.check_criteria.check_criteria_evaluator import (
    CheckCriteriaEvaluator,
    CheckCriteriaEvaluatorConfig,
)

async def main() -> None:
    config = CheckCriteriaEvaluatorConfig(
        criteria=[
            "The response should be exactly one paragraph",
            "The response should end with a question",
        ],
        passed_threshold=75,  # Criteria scoring below 75% will be included in feedback
        model="gpt-5-mini",
    )
    evaluator = CheckCriteriaEvaluator(config=config)
    messages = [
        {"role": "user", "content": "What is a programming language?"},
        {
            "role": "assistant",
            "content": "A programming language is a formal system of instructions that computers can execute. Popular examples include Python, JavaScript, and Java. Each language has its own syntax and use cases. What type of programming are you interested in learning?"
        }
    ]
    
    result = await evaluator.evaluate(messages, tools=[])
    print(f"Score: {result.score:.1f}%")
    print(f"Feedback: {result.feedback}")
    for eval in result.metadata["criteria_evaluations"]:
        print(f"- {eval['criterion']}: {eval['probability']*100:.0f}%")

asyncio.run(main())
```


### Claim Verification

Verifies factual claims in text against source context.
Implemented as AsyncGenerator that yields partial results as claims are verified.

This evaluation is based on the following two papers: [Claimify](https://arxiv.org/abs/2502.10855) and [VeriTrail](https://arxiv.org/abs/2505.21786).
This is not an official implementation of either and please cite the original papers if you use this evaluation in your work.

**Metric**: Number of verified claims / (total number of claims - number of "open-domain" claims)

```python
import asyncio

from eval_recipes.evaluations.claim_verification.claim_verification_evaluator import (
    ClaimVerificationEvaluator,
    ClaimVerificationEvaluatorConfig,
    InputClaimVerificationEvaluator,
    InputContext,
)

async def main() -> None:
    input_data = InputClaimVerificationEvaluator(
        text="Paris is the capital of France. It has 12 million residents.",
        user_question="Tell me about Paris",
        source_context=[
            InputContext(
                source_id="1",
                title="Wikipedia",
                content="Paris is the capital city of France with 2.1 million inhabitants.",
            )
        ],
    )
    config = ClaimVerificationEvaluatorConfig()  # (optionally) configure models and other parameters here
    verifier = ClaimVerificationEvaluator(config=config)
    async for result in verifier.run(input_data):
        print(result)

asyncio.run(main())
```


### Guidance

Evaluates how gracefully an assistant handles out-of-scope requests.
Determines if requests are within capabilities and evaluates response quality for out-of-scope requests.

```python
import asyncio
from eval_recipes.evaluations.guidance.guidance_evaluator import (
    GuidanceEvaluator,
    GuidanceEvaluatorConfig,
    InputGuidanceEval,
)

async def main() -> None:
    input_data = InputGuidanceEval(
        conversation_history_full="""System: You can help with text tasks.
User: Can you create an Excel spreadsheet for me?
Assistant: I cannot create Excel files, but I can help you create a CSV text file that can be opened in Excel.""",
        conversation_history_beginning_turn="""System: You can help with text tasks.
User: Can you create an Excel spreadsheet for me?""",
    )
    config = GuidanceEvaluatorConfig(
        capability_manifest="## Capabilities\n- Create and edit text files\n- Cannot create binary files like Excel spreadsheets"
    )  # (recommended) provide the capability manifest or it will be auto-generated
    evaluator = GuidanceEvaluator(config=config)
    result = await evaluator.run(input_data)
    print(result)

asyncio.run(main())
```

The `generate_capability_manifest` function helps create the capability_manifest from system prompts and tool definitions
This is useful for preprocessing noisy system prompts into clear capability descriptions.

```python
import asyncio
from openai.types.chat.chat_completion_tool_param import ChatCompletionToolParam
from eval_recipes.evaluations.guidance.guidance_evaluator import generate_capability_manifest

async def main() -> None:
    system_prompt = "You are a helpful assistant that can search the web"
    tools = [
        ChatCompletionToolParam(
            {"type": "function", "function": {"name": "calculator", "description": "Perform mathematical calculations"}}
        )
    ]
    manifest = await generate_capability_manifest(
        system_prompt=system_prompt, tools=tools, provider="openai", model="gpt-5"
    )
    print(manifest)

asyncio.run(main())
```


### Preference Adherence

Evaluates how well an assistant adheres to user preferences.
It first extracts user preferences from messages and then evaluates adherence to **each** of them.

**Metric**: Number of preferences adhered to / total number of preferences

```python
import asyncio

from eval_recipes.evaluations.preference_adherence.preference_adherence_evaluator import (
    InputUserPreferences,
    PreferenceAdherenceEvaluator,
)
from eval_recipes.schemas import BaseEvaluatorConfig

async def main() -> None:
    input_data = InputUserPreferences(
        conversation_history_beginning_turn="""System: You are a helpful assistant. You are concise and avoid emojis in your response
User: What is Python?""",
        conversation_history_full="""System: You are a helpful assistant. You are concise and avoid emojis in your response
User: What is Python?
Assistant: Python is a high-level, interpreted programming language known for simplicity and readability.""",
    )
    config = BaseEvaluatorConfig(model="gpt-5-mini")
    evaluator = PreferenceAdherenceEvaluator(config=config)
    result = await evaluator.run(input_data)
    print(result)

asyncio.run(main())
```


### Tool Usage

Evaluates whether an assistant correctly uses available tools.
Calculates probability that each tool should be called based on conversation context.

**Metric**
- If no tools were called (was_called=False for all)
  - Each probability should be below its threshold. If so, return 100, 0 otherwise.
- If was_called=True for any tool:
  - Check **any** of those tool's probability is above its threshold, if so return 100.
  - Otherwise if none of the "was_called=True" have a probability over the threshold, return 0. (This implies a tool was called when it should not have been)

```python
import asyncio
from eval_recipes.evaluations.tool_usage.tool_usage_evaluator import (
    InputTool,
    InputToolUsageEvaluator,
    ToolUsageEvaluator,
    ToolUsageEvaluatorConfig,
)

async def main() -> None:
    input_data = InputToolUsageEvaluator(
        tools=[
            InputTool(
                tool_name="search",
                tool_text="Search for information on the web",
                was_called=False,
                threshold=50,
            ),
            InputTool(
                tool_name="calculator",
                tool_text="Perform mathematical calculations",
                was_called=True,
                threshold=50,
            ),
        ],
        conversation_history_full="User: What is 15% of 200?\nAssistant: I'll calculate that for you.",
    )
    config = ToolUsageEvaluatorConfig()  # (optionally) configure models and tool thresholds here
    evaluator = ToolUsageEvaluator(config=config)
    result = await evaluator.run(input_data)
    print(result)

asyncio.run(main())
```


### Semantic Test

Evaluates agent work by using an AI agent to audit deliverables against custom steps and rubrics.
The auditor agent explores a working directory, follows specified steps, and completes a structured rubric.

**Metric**: Score extracted from the completed rubric (0-100).

```python
import asyncio
from pathlib import Path

from eval_recipes.evaluations.semantic_test.semantic_test_evaluator import (
    SemanticTestEvaluator,
    SemanticTestEvaluatorConfig,
)

async def main() -> None:
    # Setup: working_dir should contain the agent's deliverables
    working_dir = Path("/path/to/agent/work")

    context = "Create a Python script that calculates fibonacci numbers using recursion"

    steps = """1. Check if the Python file exists
1. Read the file and verify it implements fibonacci using recursion
2. Test the implementation by running it"""

    rubric = {
        "file_exists": "20 points boolean - does the file exist?",
        "uses_recursion": "40 points boolean - does it use recursion?",
        "works_correctly": "40 points boolean - does it produce correct results?",
        "score": "number (0-100) - total score based on criteria above",
    }

    config = SemanticTestEvaluatorConfig(
        working_dir=working_dir,
        steps=steps,
        rubric=rubric,
        context=context,
    )
    evaluator = SemanticTestEvaluator(config=config)
    result = await evaluator.run(
        working_dir=working_dir,
        steps=steps,
        rubric=rubric,
        context=context,
    )
    print(f"Score: {result.score:.1f}")
    print(f"Feedback: {result.feedback}")
    print(f"Metadata: {result.metadata}")

asyncio.run(main())
```

## docs/BENCHMARKING.md

# Benchmarking

This module provides a benchmarking harness for evaluating AI agents within isolated Docker containers.


## Installation

```bash
# Install prerequisites below first.
# With uv (add to project dependencies, pinned to a release tag)
uv add "eval-recipes @ git+https://github.com/microsoft/eval-recipes@v0.0.31"

# With pip
pip install "git+https://github.com/microsoft/eval-recipes@v0.0.31"
```


## Prerequisites

- [uv](https://docs.astral.sh/uv/getting-started/installation/)
- Install [Docker Desktop](https://docs.docker.com/desktop/) for work on systems running Windows* or [Docker Engine](https://docs.docker.com/engine/install/ubuntu/) on setups like WSL 2.
  - After installing Docker Engine on WSL 2, ensure your user has docker permissions by running:
    - `sudo usermod -aG docker $USER`
    - `newgrp docker`
- The Claude Agent SDK which requires setting up [Claude Code](https://docs.claude.com/en/docs/claude-code/overview)
- [`ANTHROPIC_API_KEY`](https://platform.claude.com/docs/en/get-started) for the Claude Agent SDK.
- [`OPENAI_API_KEY`](https://platform.openai.com/api-keys) if using agent continuation (see parameters below, or running tasks that requires it as a dependency).

* All features may not currently work on Windows due to Claude Agent SDK limitations.


## Usage

1. Create agent definition(s). Examples are provided in [data/agents](../data/agents) and described further [below](#agent-definitions).
1. Create task definition(s). Examples are provided in [data/tasks](../data/tasks) and described further [below](#task-definitions).
1. Create a run configuration. Examples are provided in [data/eval-setups](../data/eval-setups) and described further [below](#run-configurations).

```python
import yaml
from eval_recipes.benchmarking.harness import Harness
from eval_recipes.benchmarking.schemas import ScoreRunSpec

with Path("score-default.yaml").open(encoding="utf-8") as f:
    run_definition = ScoreRunSpec(**yaml.safe_load(f))

harness = Harness(
    agents_dir=Path("data/agents"),
    tasks_dir=Path("data/tasks"),
    run_definition=run_definition,
)
asyncio.run(harness.run())
```

Comparison based evaluations currently are separate from score based evaluations.

```python
import yaml
from eval_recipes.benchmarking.harness_comparison import ComparisonHarness
from eval_recipes.benchmarking.schemas import ComparisonTaskSpec

with Path("comparison-default.yaml").open(encoding="utf-8") as f:
    data = yaml.safe_load(f)
    specs = [ComparisonTaskSpec(task_name=c["task"], agent_names=c["agents"]) for c in data["comparisons"]]

harness = ComparisonHarness(
    agents_dir=Path("data/agents"),
    tasks_dir=Path("data/tasks"),
)

asyncio.run(harness.run(specs))
```


## Agent Definitions

Each agent is a subdirectory containing the files needed to install and run the agent:

```
agent_id/                         # Agent directory
  agent.yaml                      # Agent configuration
  install.dockerfile              # Docker commands to install the agent
  command_template.txt            # Liquid template for the command to start a task with the agent
  command_template_continue.txt   # (Optional) Template for agent continuation when follow-up is needed. This command must continue from the previous session/conversation.
  data/                           # (Optional) Agent-specific data files used during installation or runtime
```

### `agent.yaml`

Configuration file for the agent. All fields are optional.

```yaml
# Environment variables passed into the container.
# Sourced from the harness `environment` parameter.
required_env_vars:
  - ANTHROPIC_API_KEY
  - OPENAI_API_KEY

# Absolute path to local source code for development.
local_source_path: /path/to/source
```

### `install.dockerfile`

Docker commands to install the agent. These are injected into the [base image](../eval_recipes/benchmarking/base.dockerfile).

```dockerfile
# Example: Install GitHub Copilot CLI
RUN npm install -g @github/copilot
RUN copilot --version
```

### `command_template.txt`

[Python Liquid](https://github.com/jg-rp/liquid) template for the command to run the agent. The `{{task_instructions}}` variable contains the task instructions from `instructions.txt`.

```
copilot -p "{{task_instructions}}" --allow-all-tools
```

### `command_template_continue.txt`

Optional. Liquid template for continuing an agent session when follow-up is needed. The `{{task_instructions}}` variable contains the continuation prompt. This command must resume the previous conversation.

```
copilot -p "{{task_instructions}}" --continue --allow-all-tools
```

### `data/`

Optional. Directory containing agent-specific data files. Contents are copied to `/project` in the container before the agent runs.

### Defining Local Agents

For development and testing, you can create agent variants that use local source code instead of remote repositories. 

1. Add `agent.yaml` with `local_source_path` pointing to your local source:
   ```yaml
   local_source_path: /absolute/path/to/your/agent/source
   ```
1. Create `install.dockerfile` that installs from `/tmp/agent_source/` (where source is automatically copied)
1. By default it ignores files in `.gitignore` if present (otherwise excludes `.git`, `.venv`, `__pycache__`, etc.)


## Task Definitions

Each task is a subdirectory containing the files needed to define the task and test the agent's solution.

```
task_id/
  task.yaml            # Task configuration (required)
  instructions.txt     # Instructions given to the agent (required)
  test.py              # Python script to test the agent's solution (required for score-based tasks)
  setup.dockerfile     # (Optional) Docker commands to set up the task environment
  task_time_data/      # (Optional) Data files copied before agent runs
  test_time_data/      # (Optional) Data files copied before tests run
```

### `task.yaml`

Configuration file for the task.

```yaml
# Required
task_info:
  difficulty: medium                                   # "easy", "medium", or "hard"
  non_deterministic_evals: true                        # Whether evals use LLMs or not (default: false)
  categories:                                          # Optional category tags
    - cli_tool
    - writing

# Optional
timeout: 5400                                          # Timeout in seconds (default: 1800)
required_env_vars:                                     # Environment variables needed by the task
  - ANTHROPIC_API_KEY
test_command: uv run --no-project /project/test.py     # Command to run tests (default shown)

# For comparison-based tasks
eval_type: comparison                                  # "score", "comparison", or "both" (default: "score")
comparison_eval:
  guidelines: |                                        # Evaluation guidelines for the comparison judge
    Focus on clarity and completeness of the response.
```

### `instructions.txt`

The task prompt given to the agent. This is passed to the agent via the `{{task_instructions}}` template variable in `command_template.txt`.

### `test.py`

Python script that evaluates the agent's work. The script runs inside the container after the agent completes. 
Uses `semantic_test()` for LLM-based evaluation (eval-recipes is by default available when the script runs) or direct file checks for deterministic tests.

The script must output a JSON to following path `{output_dir}/.eval_recipes_test_results_{test_id}.json` and the following format:

```json
{"score": 85.0, "metadata": {"field": "value"}}
```

The `write_test_result()` utility handles writing this file in the correct format. 
See the following complete example:

```python
import asyncio
import sys
from pathlib import Path

import click
from eval_recipes.benchmarking.semantic_test import semantic_test
from eval_recipes.benchmarking.test_utils import (
    get_instructions_from_file_or_default,
    get_test_id_from_env_or_default,
    write_test_result,
)

STEPS = """1. Explore the /project directory to understand what the agent created
2. Verify the required files exist
3. Check that the implementation meets the requirements"""

RUBRIC = {
    "files_exist": "bool - Do all required files exist?",
    "implementation_correct": "str - Assessment of the implementation",
    "score": "float - Overall score from 0-100",
}

@click.command()
@click.option("--test-id", default=lambda: get_test_id_from_env_or_default("dev"))
@click.option("--output-dir", type=click.Path(path_type=Path), default=lambda: Path(__file__).parents[0])
@click.option("--instructions-file", type=click.Path(path_type=Path), default=None)
def main(test_id: str, output_dir: Path, instructions_file: Path | None) -> int:
    return asyncio.run(run_test(test_id, output_dir, instructions_file))

async def run_test(test_id: str, output_dir: Path, instructions_file: Path | None) -> int:
    instructions = get_instructions_from_file_or_default(instructions_file)

    result = await semantic_test(
        steps=STEPS,
        rubric=RUBRIC,
        context=instructions,
        working_dir=Path("/project"),
    )

    write_test_result(output_dir, test_id, result.score, result.metadata)
    return 0

if __name__ == "__main__":
    sys.exit(main())
```

### `setup.dockerfile`

Optional. Docker commands to set up task-specific dependencies. Injected into the [base image](../eval_recipes/benchmarking/base.dockerfile) after the agent installation.

```dockerfile
# Example: Install Ollama for embedding tasks
RUN curl -fsSL https://ollama.com/install.sh | sh
RUN nohup ollama serve > /dev/null 2>&1 & sleep 5 && ollama pull embeddinggemma:300m-qat-q8_0
```

### `task_time_data/`

Optional. Directory containing files the agent needs to complete the task (e.g., PDFs, source code archives). Contents are copied to `/project` before the agent runs.

### `test_time_data/`

Optional. Directory containing files needed only for testing (e.g., expected outputs, test inputs). Contents are copied to `/project` before tests run, but after the agent completes.


## Run Configurations

YAML files that define which agents to run on which tasks. Agent and task names must match directory names in `agents_dir` and `tasks_dir` passed to the harness.

### Score-Based Configuration

```yaml
type: score
definitions:
  - agent: claude_code
    trials: 3                        # Default trials for this agent's tasks
    tasks:
      - task: email_drafting
      - task: arxiv_paper_summarizer
        trials: 5                    # Override trials for this specific task
  - agent: gh_cli
    trials: 3
    tasks:
      - task: email_drafting
```

### Comparison-Based Configuration

Each comparison can include any number of agents (minimum 2). The harness runs each agent on the task and uses an agent judge to rank their outputs.

```yaml
comparisons:
  - task: ppt-1
    agents:
      - claude_code
      - gh_cli
      - openai_codex
  - task: ppt-2
    agents:
      - claude_code
      - gh_cli
```


## Harnesses

Harnesses orchestrate the benchmark run: they build Docker images for each agent-task combination, run agents in isolated containers, execute test scripts to evaluate results, and generate HTML reports with scores and analysis.

### Score Harness

[Source](../eval_recipes/benchmarking/harness.py)

```python
from eval_recipes.benchmarking.harness import Harness

harness = Harness(
    agents_dir=Path,                  # Directory containing agent definitions
    tasks_dir=Path,                   # Directory containing task definitions
    run_definition=ScoreRunSpec,      # Parsed run configuration
    runs_dir=Path | None,             # Output directory (default: .benchmark_results)
    environment=dict | None,          # Environment variables for containers
    max_parallel_trials=5,            # Maximum concurrent trials
    continuation_provider="none",     # "openai", "azure_openai", or "none"
    continuation_model="gpt-5",       # Model for continuation prompts
    eval_recipes_version="...",       # Version of eval-recipes in containers
    report_score_threshold=85.0,      # Generate failure reports below this score
)

await harness.run()
```

### Comparison Harness

[Source](../eval_recipes/benchmarking/harness_comparison.py)

```python
from eval_recipes.benchmarking.harness_comparison import ComparisonHarness

harness = ComparisonHarness(
    agents_dir=Path,                  # Directory containing agent definitions
    tasks_dir=Path,                   # Directory containing task definitions
    runs_dir=Path | None,             # Output directory (default: .comparison_results)
    environment=dict | None,          # Environment variables for containers
    max_parallel=5,                   # Maximum concurrent jobs
    comparison_runs=3,                # Number of comparison runs per task
    continuation_provider="none",     # "openai", "azure_openai", or "none"
    continuation_model="gpt-5",       # Model for continuation prompts
    eval_recipes_version="...",       # Version of eval-recipes in containers
    report_score_threshold=85.0,      # Score threshold for reports
)

await harness.run(comparison_specs)   # Pass list of ComparisonTaskSpec
```


## Semantic Test

LLM-based evaluation functions that use Claude to audit agent work. Used within `test.py` scripts.

### `semantic_test`

[Source](../eval_recipes/benchmarking/semantic_test.py)

Audits a single agent's work against a rubric.

```python
from eval_recipes.benchmarking.semantic_test import semantic_test

result = await semantic_test(
    steps="1. Explore /project\n2. Check if README exists\n3. Run the tests",
    rubric={
        "readme_exists": "bool - Does README.md exist?",
        "tests_pass": "bool - Do the tests pass?",
        "score": "float - Overall score 0-100",  # Required field
    },
    context="The agent was asked to create a CLI tool...",
    working_dir=Path("/project"),
)

# result.score: float (0-100)
# result.metadata: dict (other rubric fields)
```

### `semantic_test_comparison`

[Source](../eval_recipes/benchmarking/semantic_test_comparison.py)

Compares multiple agents' work on the same task using blind evaluation. Directories are anonymized before comparison.

```python
from eval_recipes.benchmarking.semantic_test_comparison import semantic_test_comparison

result = await semantic_test_comparison(
    original_task="Create a CLI tool that...",
    directories=[
        Path("/outputs/agent_1/project"),
        Path("/outputs/agent_2/project"),
        Path("/outputs/agent_3/project"),
    ],
    guidelines="Focus on code quality and completeness",  # Optional
    log_file=Path("comparison.log"),  # Optional
)

# result.reasoning: str (bullet-point analysis)
# result.rankings: list[int] (indices ordered best to worst)
# result.anonymous_to_index: dict (maps anonymized names to indices)
```


## Third-Party Benchmarks

Scripts for integrating third-party benchmarks are located in `scripts/third_party_benchmarks/`. These scripts download benchmark data from external sources and convert them into eval-recipes score-based task format.

### ARC-AGI-2

[ARC-AGI-2](https://github.com/arcprize/ARC-AGI-2/tree/main) is a general artificial intelligence benchmark focused on abstract reasoning. Tasks involve analyzing input/output grid pairs to discover transformation patterns, then applying those patterns to new test inputs.
A set of sample tasks can be created using this script:

```bash
uv run scripts/third_party_benchmarks/setup_arc_agi_2.py --num-tasks 10 --seed 42 --output-dir data/tasks --clean
```

### OpenAI FrontierScience

[FrontierScience](https://huggingface.co/datasets/openai/frontierscience) is a benchmark that evaluates AI capabilities for expert-level scientific reasoning across physics, chemistry, and biology.

```bash
uv run scripts/third_party_benchmarks/setup_frontier_science.py --num-tasks 10 --seed 42 --output-dir data/tasks --clean
```

## Notes

- You may want to prune your Docker images and containers periodically to save space. Containers/images can hang around when runs are unexpectedly interrupted.

## eval_recipes/evaluate.py

# Copyright (c) Microsoft. All rights reserved.

import asyncio

from openai.types.chat.chat_completion_tool_param import ChatCompletionToolParam
from openai.types.responses import ResponseInputParam

from eval_recipes.evaluations.check_criteria.check_criteria_evaluator import (
    CheckCriteriaEvaluator,
    CheckCriteriaEvaluatorConfig,
)
from eval_recipes.evaluations.claim_verification.claim_verification_evaluator import (
    ClaimVerificationEvaluator,
    ClaimVerificationEvaluatorConfig,
)
from eval_recipes.evaluations.guidance.guidance_evaluator import GuidanceEvaluator, GuidanceEvaluatorConfig
from eval_recipes.evaluations.preference_adherence.preference_adherence_evaluator import PreferenceAdherenceEvaluator
from eval_recipes.evaluations.tool_usage.tool_usage_evaluator import ToolUsageEvaluator, ToolUsageEvaluatorConfig
from eval_recipes.schemas import BaseEvaluatorConfig, EvaluationOutput, EvaluatorProtocol


async def evaluate(
    messages: ResponseInputParam,
    tools: list[ChatCompletionToolParam],
    evaluations: list[str | type[EvaluatorProtocol]],
    evaluation_configs: dict[str, BaseEvaluatorConfig] | None = None,
    max_concurrency: int = 1,
) -> list[EvaluationOutput]:
    """
    Evaluates the model's performance based on the provided messages and tools over the specified evaluations.

    Args:
        messages: OpenAI responses API input messages
        tools: OpenAI tool definitions
        evaluations: The list of evaluation names or custom evaluator classes to perform.
          Built-in options are: "claim_verification", "tool_usage", "guidance", and "preference_adherence".
          You can also pass custom evaluator classes that implement the EvaluatorProtocol.
        evaluation_configs: Optional configs for each evaluation.
          Keys should be the built-in evaluation names or custom evaluator class names.
          If not provided, defaults will be used.
        max_concurrency: Maximum number of evaluations to run concurrently. Default is 1 (sequential).

    Returns:
        A list of EvaluationOutput objects containing the evaluation results.
        Each object includes the evaluation name, score, and optional metadata specific to that evaluation.
    """
    if evaluation_configs is None:
        evaluation_configs = {}

    evaluator_map: dict[str, tuple[type, type[BaseEvaluatorConfig]]] = {
        "guidance": (GuidanceEvaluator, GuidanceEvaluatorConfig),
        "preference_adherence": (PreferenceAdherenceEvaluator, BaseEvaluatorConfig),
        "tool_usage": (ToolUsageEvaluator, ToolUsageEvaluatorConfig),
        "claim_verification": (ClaimVerificationEvaluator, ClaimVerificationEvaluatorConfig),
        "check_criteria": (CheckCriteriaEvaluator, CheckCriteriaEvaluatorConfig),
    }

    semaphore = asyncio.Semaphore(max_concurrency)

    async def run_evaluation(
        eval_item: str | type[EvaluatorProtocol],
    ) -> EvaluationOutput | None:
        async with semaphore:
            # Handle string evaluation names (built-in evaluators)
            if isinstance(eval_item, str):
                if eval_item not in evaluator_map:
                    return None
                evaluator_class, config_type = evaluator_map[eval_item]
                config = evaluation_configs.get(eval_item)
                validated_config = config if isinstance(config, config_type) else None
                evaluator = evaluator_class(config=validated_config)
            # Handle custom evaluator classes
            else:
                # Check if the class implements the required protocol
                if not isinstance(eval_item, type) or not issubclass(eval_item, EvaluatorProtocol):
                    raise ValueError(f"Custom evaluator {eval_item} must be a class that implements EvaluatorProtocol")
                # Get config using the class name
                class_name = eval_item.__name__
                config = evaluation_configs.get(class_name)
                evaluator = eval_item(config=config)

            return await evaluator.evaluate(messages, tools)

    tasks = [run_evaluation(eval_item) for eval_item in evaluations]
    results = await asyncio.gather(*tasks)
    return [result for result in results if result is not None]

## eval_recipes/schemas.py

# Copyright (c) Microsoft. All rights reserved.


from typing import Any, Literal, Protocol, runtime_checkable

from openai.types.chat.chat_completion_tool_param import ChatCompletionToolParam
from openai.types.responses import ResponseInputParam
from pydantic import BaseModel


class EvaluationOutput(BaseModel):
    eval_name: str  # Name of the evaluation
    applicable: bool  # Was the evaluation applicable
    score: float  # Score from 0 to 100
    feedback: str | None = None  # Feedback that can be used as part of another system to self-improve the response.
    metadata: dict[str, Any] = {}  # Any additional metadata the evaluation may generate.


# region Eval Configurations


class BaseEvaluatorConfig(BaseModel):
    provider: Literal["openai", "azure_openai"] = "openai"
    model: Literal["gpt-5", "gpt-5-mini", "gpt-5-nano", "o3", "o4-mini"] = "gpt-5"


# endregion


@runtime_checkable
class EvaluatorProtocol(Protocol):
    """Protocol for custom evaluator classes."""

    def __init__(self, config: BaseEvaluatorConfig | None = None) -> None:
        """Initialize the evaluator with an optional configuration.
        If config is not provided, it should be instantiated with defaults.
        """
        ...

    async def evaluate(self, messages: ResponseInputParam, tools: list[ChatCompletionToolParam]) -> EvaluationOutput:
        """Evaluate messages and tools, returning an EvaluationOutput."""
        ...

## scripts/run_benchmarks.py

# Copyright (c) Microsoft. All rights reserved

import asyncio
from datetime import UTC, datetime
import os
from pathlib import Path
from typing import Literal, cast

import click
from dotenv import load_dotenv
from loguru import logger
import yaml

from eval_recipes.benchmarking.harness import Harness
from eval_recipes.benchmarking.schemas import ScoreRunSpec

load_dotenv()


@click.command()
@click.option(
    "--config",
    "config_file",
    type=click.Path(exists=True, dir_okay=False, path_type=Path),
    default=lambda: Path(__file__).parents[1] / "data" / "eval-setups" / "score-default.yaml",
    help="Path to YAML config file with run definition",
)
@click.option(
    "--agents-dir",
    type=click.Path(exists=True, file_okay=False, path_type=Path),
    default=lambda: Path(__file__).parents[1] / "data" / "agents",
    help="Directory containing agent configurations",
)
@click.option(
    "--tasks-dir",
    type=click.Path(exists=True, file_okay=False, path_type=Path),
    default=lambda: Path(__file__).parents[1] / "data" / "tasks",
    help="Directory containing task definitions",
)
@click.option(
    "--runs-dir",
    type=click.Path(file_okay=False, path_type=Path),
    default=None,
    help="Run directory. If not provided, creates a new timestamped dir. If provided and exists, resumes.",
)
@click.option(
    "--max-parallel-trials",
    type=int,
    default=20,
    help="Maximum number of trials to run in parallel",
)
@click.option(
    "--continuation-provider",
    type=click.Choice(["openai", "azure_openai", "none"], case_sensitive=False),
    default="openai",
    help="LLM provider for agent continuation ('none' to disable)",
)
@click.option(
    "--continuation-model",
    type=click.Choice(["gpt-5", "gpt-5.1"], case_sensitive=False),
    default="gpt-5",
    help="Model to use for agent continuation decisions",
)
@click.option(
    "--report-score-threshold",
    type=float,
    default=85.0,
    help="Minimum score threshold to skip report generation (reports generated for scores below this)",
)
def main(
    config_file: Path,
    agents_dir: Path,
    tasks_dir: Path,
    runs_dir: Path | None,
    max_parallel_trials: int,
    continuation_provider: str,
    continuation_model: str,
    report_score_threshold: float,
) -> None:
    # Load run definition from config file
    with config_file.open(encoding="utf-8") as f:
        config = yaml.safe_load(f)

    run_definition = ScoreRunSpec.model_validate(config)
    logger.info(f"Loaded run definition from {config_file}")

    if runs_dir is None:
        base_dir = Path(__file__).parents[1] / ".benchmark_results"
        timestamp = datetime.now(UTC).strftime("%Y-%m-%d_%H-%M-%S-%f")[:-3]
        runs_dir = base_dir / timestamp
        logger.info(f"Starting new run: {runs_dir}")
    else:
        if runs_dir.exists() and (runs_dir / "jobs.db").exists():
            logger.info(f"Resuming existing run: {runs_dir}")
        else:
            logger.info(f"Starting new run: {runs_dir}")

    runs_dir.mkdir(parents=True, exist_ok=True)
    log_file = runs_dir / "benchmark_run_job.log"
    logger.add(log_file, format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}")
    logger.info(f"Logging to {log_file}")

    harness = Harness(
        agents_dir=agents_dir,
        tasks_dir=tasks_dir,
        run_definition=run_definition,
        runs_dir=runs_dir,
        environment={
            "ANTHROPIC_API_KEY": os.environ.get("ANTHROPIC_API_KEY", ""),
            "OPENAI_API_KEY": os.environ.get("OPENAI_API_KEY", ""),
            "GITHUB_TOKEN": os.environ.get("GITHUB_TOKEN", ""),
            "AZURE_OPENAI_ENDPOINT": os.environ.get("AZURE_OPENAI_ENDPOINT", ""),
            "AZURE_OPENAI_VERSION": os.environ.get("AZURE_OPENAI_VERSION", ""),
        },
        max_parallel_trials=max_parallel_trials,
        continuation_provider=cast(Literal["openai", "azure_openai", "none"], continuation_provider),
        continuation_model=cast(Literal["gpt-5", "gpt-5.1"], continuation_model),
        report_score_threshold=report_score_threshold,
    )
    asyncio.run(harness.run())


if __name__ == "__main__":
    main()
